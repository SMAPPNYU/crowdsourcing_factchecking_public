{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author of code: William Godel \n",
    "\n",
    "Date: 07/02\n",
    "\n",
    "Purpose: to train ML models, and build select crowds data\n",
    "\n",
    "## Data IN: \n",
    "\n",
    "train_data_large.csv\n",
    "\n",
    "val_data_large.csv\n",
    "\n",
    "train_data_large_covid.csv\n",
    "\n",
    "val_data_large_covid.csv\n",
    "\n",
    "train_data_large_noncovid.csv\n",
    "\n",
    "val_data_large_noncovid.csv\n",
    "\n",
    "\n",
    "## Data OUT:\n",
    "\n",
    "### models\n",
    "\n",
    "grid_rf.p\n",
    "\n",
    "grid_en.p\n",
    "\n",
    "model.pt\n",
    "\n",
    "model_nn_highpol.pt\n",
    "\n",
    "\n",
    "Machine: My laptop or Imac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "laptop = True\n",
    "import pickle\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(filename='ML.log',level=logging.DEBUG)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import ElasticNet, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "from ml_functions import feature_creation, conf_eval, data_prep, test_model\n",
    "\n",
    "from functions import count_mode, bayes_probs, bayes_binary\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#crowd_size, feature_transform\n",
    "from path import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(prepared_data + \"train_data_large.csv\")\n",
    "val_data = pd.read_csv(prepared_data + \"val_data_large.csv\")\n",
    "\n",
    "train_data_covid = pd.read_csv(prepared_data + \"train_data_large_covid.csv\")\n",
    "val_data_covid = pd.read_csv(prepared_data + \"val_data_large_covid.csv\")\n",
    "\n",
    "train_data_noncovid = pd.read_csv(prepared_data + \"train_data_large_noncovid.csv\")\n",
    "val_data_noncovid = pd.read_csv(prepared_data + \"val_data_large_noncovid.csv\")\n",
    "\n",
    "\n",
    "train_data_large = train_data.append(train_data_covid)\n",
    "train_data_large = train_data_large.append(train_data_noncovid)\n",
    "\n",
    "train_data_large.reset_index(inplace = True, drop = True)\n",
    "\n",
    "\n",
    "val_data_large = val_data.append(val_data_covid)\n",
    "val_data_large = val_data_large.append(val_data_noncovid)\n",
    "\n",
    "val_data_large.reset_index(inplace = True, drop = True)\n",
    "\n",
    "\n",
    "train_target_large = train_data_large['mode'] == 'FM'\n",
    "val_target_large = val_data_large['mode'] == 'FM'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Feature creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building ML pipelines:\n",
    "\n",
    "1. Elastic net \n",
    "2. Random Forest\n",
    "3. Gradient Boosted Trees\n",
    "4. NN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the data\n",
    "X_data = feature_creation(train_data_large)\n",
    "Y_data = train_target_large\n",
    "\n",
    "X_data_val = feature_creation(val_data_large)\n",
    "Y_data_val = val_target_large\n",
    "\n",
    "X_data_all = X_data.append(X_data_val)\n",
    "Y_data_all = Y_data.append(Y_data_val)\n",
    "\n",
    "X_data_all.reset_index(inplace = True, drop = True)\n",
    "Y_data_all.reset_index(inplace = True, drop = True)\n",
    "\n",
    "fold_list = [-1 for x in range(X_data.shape[0])]\n",
    "fold_list.extend([0 for x in range(X_data_val.shape[0])])\n",
    "\n",
    "test_fold = np.array(fold_list)\n",
    "ps = PredefinedSplit(test_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = []\n",
    "\n",
    "resp_var = [x for x in X_data.columns if 'resp_veracity_' in x]\n",
    "new_cols = [x for x in X_data.columns if 'new' in x]\n",
    "\n",
    "numeric_features.extend(resp_var)\n",
    "numeric_features.extend(new_cols)\n",
    "numeric_features.extend([\"crowd_means\",'crowd_median','crowd_full_range', 'crowd_IQR_range', \\\n",
    "                         'crowd_variance', 'crowd_bayes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0])),\n",
       "             error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('preprocessor',\n",
       "                                        ColumnTransformer(n_jobs=None,\n",
       "                                                          remainder='drop',\n",
       "                                                          sparse_threshold=0.3,\n",
       "                                                          transformer_weights=None,\n",
       "                                                          transformers=[('num',\n",
       "                                                                         Pipeline(memory=None,\n",
       "                                                                                  steps=[('scaler',\n",
       "                                                                                          StandardScaler(copy=True,\n",
       "                                                                                                         with_mean=True,\n",
       "                                                                                                         with_s...\n",
       "                                                               random_state=None,\n",
       "                                                               verbose=0,\n",
       "                                                               warm_start=False))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'classifier__max_depth': [1, 2, 3, 4, 5, 10, 20, None],\n",
       "                         'classifier__min_samples_leaf': [1, 2, 3, 5, 10],\n",
       "                         'classifier__min_samples_split': [2, 3, 5, 10],\n",
       "                         'classifier__n_estimators': [500, 1000, 2000]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[('num', numeric_transformer, numeric_features)])\n",
    "\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', RandomForestClassifier())])\n",
    "\n",
    "parameters = {'classifier__max_depth':[1,2,3,4,5,10,20,None], \\\n",
    "               'classifier__min_samples_split':[2,3,5,10], \\\n",
    "               'classifier__min_samples_leaf':[1,2,3,5,10], \\\n",
    "               'classifier__n_estimators':[500,1000,2000]}\n",
    "\n",
    "\n",
    "grid_rf = GridSearchCV(clf, scoring = 'accuracy', param_grid=parameters, cv=ps)\n",
    "\n",
    "grid_rf.fit(X_data_all,Y_data_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(grid_rf.predict(X_data_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5700952380952381"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_rf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5902056074766355"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1- Y_data_all.sum()/Y_data_all.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>resp_veracity_0</th>\n",
       "      <th>resp_veracity_1</th>\n",
       "      <th>resp_veracity_2</th>\n",
       "      <th>resp_veracity_3</th>\n",
       "      <th>resp_veracity_4</th>\n",
       "      <th>resp_veracity_5</th>\n",
       "      <th>resp_veracity_6</th>\n",
       "      <th>resp_veracity_7</th>\n",
       "      <th>resp_veracity_8</th>\n",
       "      <th>resp_veracity_9</th>\n",
       "      <th>...</th>\n",
       "      <th>resp_cat_6_f</th>\n",
       "      <th>resp_cat_6_t</th>\n",
       "      <th>resp_cat_7_f</th>\n",
       "      <th>resp_cat_7_t</th>\n",
       "      <th>resp_cat_8_f</th>\n",
       "      <th>resp_cat_8_t</th>\n",
       "      <th>resp_cat_9_f</th>\n",
       "      <th>resp_cat_9_t</th>\n",
       "      <th>crowd_mode_f</th>\n",
       "      <th>crowd_mode_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   resp_veracity_0  resp_veracity_1  resp_veracity_2  resp_veracity_3  \\\n",
       "0              6.0              4.0              3.0              4.0   \n",
       "1              4.0              4.0              4.0              4.0   \n",
       "2              4.0              2.0              4.0              4.0   \n",
       "3              5.0              5.0              7.0              3.0   \n",
       "4              5.0              4.0              7.0              5.0   \n",
       "\n",
       "   resp_veracity_4  resp_veracity_5  resp_veracity_6  resp_veracity_7  \\\n",
       "0              5.0              4.0              3.0              3.0   \n",
       "1              3.0              6.0              4.0              5.0   \n",
       "2              6.0              7.0              4.0              4.0   \n",
       "3              2.0              5.0              1.0              4.0   \n",
       "4              7.0              5.0              5.0              5.0   \n",
       "\n",
       "   resp_veracity_8  resp_veracity_9  ...  resp_cat_6_f  resp_cat_6_t  \\\n",
       "0              4.0              3.0  ...             0             0   \n",
       "1              6.0              5.0  ...             0             0   \n",
       "2              5.0              1.0  ...             0             1   \n",
       "3              2.0              3.0  ...             1             0   \n",
       "4              4.0              4.0  ...             0             1   \n",
       "\n",
       "   resp_cat_7_f  resp_cat_7_t  resp_cat_8_f  resp_cat_8_t  resp_cat_9_f  \\\n",
       "0             0             0             0             0             0   \n",
       "1             0             0             1             0             0   \n",
       "2             0             1             0             1             1   \n",
       "3             0             0             1             0             1   \n",
       "4             0             1             0             0             0   \n",
       "\n",
       "   resp_cat_9_t  crowd_mode_f  crowd_mode_t  \n",
       "0             0             0             0  \n",
       "1             1             1             0  \n",
       "2             0             0             1  \n",
       "3             0             0             1  \n",
       "4             0             0             1  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(grid_rf, open(models + \"grid_rf.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wpg205/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/wpg205/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/wpg205/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/wpg205/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/wpg205/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/wpg205/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/wpg205/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/wpg205/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/wpg205/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/wpg205/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/wpg205/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/wpg205/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/wpg205/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/wpg205/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/wpg205/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/wpg205/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/wpg205/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/wpg205/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/wpg205/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/wpg205/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/wpg205/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/wpg205/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/wpg205/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/wpg205/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/wpg205/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/wpg205/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/wpg205/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/wpg205/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/wpg205/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/wpg205/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0])),\n",
       "             error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('preprocessor',\n",
       "                                        ColumnTransformer(n_jobs=None,\n",
       "                                                          remainder='drop',\n",
       "                                                          sparse_threshold=0.3,\n",
       "                                                          transformer_weights=None,\n",
       "                                                          transformers=[('num',\n",
       "                                                                         Pipeline(memory=None,\n",
       "                                                                                  steps=[('scaler',\n",
       "                                                                                          StandardScaler(copy=True,\n",
       "                                                                                                         with_mean=True,\n",
       "                                                                                                         with_s...\n",
       "                                                           multi_class='warn',\n",
       "                                                           n_jobs=None,\n",
       "                                                           penalty='elasticnet',\n",
       "                                                           random_state=None,\n",
       "                                                           solver='saga',\n",
       "                                                           tol=0.0001,\n",
       "                                                           verbose=0,\n",
       "                                                           warm_start=False))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'classifier__C': [0.1, 0.5, 1, 5, 10, 20],\n",
       "                         'classifier__l1_ratio': [0.9, 0.8, 0.05, 0.2, 0.15,\n",
       "                                                  0.1, 1]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Elastic Net\n",
    "numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[('num', numeric_transformer, numeric_features)])\n",
    "\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', LogisticRegression(penalty = 'elasticnet', solver = 'saga'))])\n",
    "\n",
    "parameters = {'classifier__l1_ratio':[0.9,.8,.05,.2,.15,0.1,1], 'classifier__C':[.1,.5,1,5,10,20]}\n",
    "\n",
    "grid_en = GridSearchCV(clf, scoring = 'accuracy', param_grid=parameters, cv=ps)\n",
    "\n",
    "grid_en.fit(X_data_all,Y_data_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(grid_en, open(models + \"grid_en.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the data\n",
    "#X_data = feature_creation(train_data_fakesource.iloc[:,:])\n",
    "#Y_data = train_target_large_lowcred[:]\n",
    "\n",
    "y_data_bin = np.where(Y_data == True, 1, 0)\n",
    "\n",
    "test = StandardScaler()\n",
    "test.fit(X_data[numeric_features])\n",
    "\n",
    "#training data\n",
    "X_train_stand = X_data.copy()\n",
    "X_train_stand[numeric_features] = test.transform(X_train_stand[numeric_features])\n",
    "\n",
    "#test data\n",
    "X_val_stand = X_data_val.copy()\n",
    "X_val_stand[numeric_features] = test.transform(X_val_stand[numeric_features])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dim):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(emb_dim, 500) \n",
    "        self.fc2 = nn.Linear(500, 100)\n",
    "        self.fc3 = nn.Linear(100, 25)\n",
    "        self.fc4 = nn.Linear(25, 25)\n",
    "        self.fc5 = nn.Linear(25, 10)\n",
    "        self.fc6 = nn.Linear(10, 1)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "class Net_dropout(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dim):\n",
    "        super(Net_dropout, self).__init__()\n",
    "        self.fc1 = nn.Linear(emb_dim, 500) \n",
    "        self.fc2 = nn.Linear(500, 100)\n",
    "        self.fc3 = nn.Linear(100, 25)\n",
    "        self.fc4 = nn.Linear(25, 25)\n",
    "        self.fc5 = nn.Linear(25, 10)\n",
    "        self.fc6 = nn.Linear(10, 1)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))  \n",
    "        x = self.dropout(x)   \n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#general model function for testing hyper parameters\n",
    "\n",
    "\n",
    "def model_train(emb_dim, model, learning_rate, \\\n",
    "                num_epochs, criterion, optimizer, \\\n",
    "                train_loader, val_loader):\n",
    "    \n",
    "\n",
    "\n",
    "    loss_vals = []\n",
    "    acc_est = []\n",
    "    train_est = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (data, labels) in enumerate(train_loader):        \n",
    "            model.train()\n",
    "            data_batch, label_batch = data, labels\n",
    "            label_batch = torch.reshape(label_batch, (-1,1))\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data_batch.float())\n",
    "            loss = criterion(outputs, label_batch.float())\n",
    "            loss_vals.append(loss/labels.size(0))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "    \n",
    "            # validate every 100 iterations\n",
    "            if i > 0 and i % 1000 == 0:\n",
    "                # validate\n",
    "                train_acc = test_model(train_loader, model)\n",
    "                train_est.append(train_acc)\n",
    "                val_acc = test_model(val_loader, model)\n",
    "                acc_est.append(val_acc)\n",
    "                #loss_vals.append(test_model_LOSS(train_loader,model))\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}],Train Acc:{}, Validation Acc: {}'.format( \n",
    "                           epoch+1, num_epochs, i+1, len(train_loader), train_acc, val_acc))\n",
    "                \n",
    "            \n",
    "    \n",
    "    return loss_vals, train_est, acc_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_prep(X_train_stand,Y_data)\n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=10,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "val_data = data_prep(X_val_stand,Y_data_val)\n",
    "valloader = torch.utils.data.DataLoader(val_data, batch_size=10,\n",
    "                                          shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/8], Step: [1001/4300],Train Acc:64.28837209302326, Validation Acc: 57.03809523809524\n",
      "Epoch: [1/8], Step: [2001/4300],Train Acc:64.92325581395349, Validation Acc: 57.94285714285714\n",
      "Epoch: [1/8], Step: [3001/4300],Train Acc:66.2, Validation Acc: 59.58095238095238\n",
      "Epoch: [1/8], Step: [4001/4300],Train Acc:67.34418604651162, Validation Acc: 62.733333333333334\n",
      "Epoch: [2/8], Step: [1001/4300],Train Acc:69.1, Validation Acc: 62.67619047619048\n",
      "Epoch: [2/8], Step: [2001/4300],Train Acc:70.58604651162791, Validation Acc: 66.61904761904762\n",
      "Epoch: [2/8], Step: [3001/4300],Train Acc:70.49767441860465, Validation Acc: 65.85714285714286\n",
      "Epoch: [2/8], Step: [4001/4300],Train Acc:70.92093023255813, Validation Acc: 66.5904761904762\n",
      "Epoch: [3/8], Step: [1001/4300],Train Acc:71.04186046511627, Validation Acc: 67.23809523809524\n",
      "Epoch: [3/8], Step: [2001/4300],Train Acc:71.52558139534884, Validation Acc: 67.60952380952381\n",
      "Epoch: [3/8], Step: [3001/4300],Train Acc:71.42325581395349, Validation Acc: 68.07619047619048\n",
      "Epoch: [3/8], Step: [4001/4300],Train Acc:71.46511627906976, Validation Acc: 66.98095238095237\n",
      "Epoch: [4/8], Step: [1001/4300],Train Acc:71.68837209302326, Validation Acc: 67.24761904761905\n",
      "Epoch: [4/8], Step: [2001/4300],Train Acc:71.8093023255814, Validation Acc: 68.29523809523809\n",
      "Epoch: [4/8], Step: [3001/4300],Train Acc:71.7139534883721, Validation Acc: 67.26666666666667\n",
      "Epoch: [4/8], Step: [4001/4300],Train Acc:71.97906976744186, Validation Acc: 68.8952380952381\n",
      "Epoch: [5/8], Step: [1001/4300],Train Acc:71.76279069767442, Validation Acc: 68.07619047619048\n",
      "Epoch: [5/8], Step: [2001/4300],Train Acc:71.98372093023256, Validation Acc: 68.0\n",
      "Epoch: [5/8], Step: [3001/4300],Train Acc:71.94186046511628, Validation Acc: 69.1047619047619\n",
      "Epoch: [5/8], Step: [4001/4300],Train Acc:71.81860465116279, Validation Acc: 67.35238095238095\n",
      "Epoch: [6/8], Step: [1001/4300],Train Acc:72.05581395348837, Validation Acc: 67.85714285714286\n",
      "Epoch: [6/8], Step: [2001/4300],Train Acc:72.02093023255814, Validation Acc: 67.44761904761904\n",
      "Epoch: [6/8], Step: [3001/4300],Train Acc:72.23488372093023, Validation Acc: 68.08571428571429\n",
      "Epoch: [6/8], Step: [4001/4300],Train Acc:72.16046511627907, Validation Acc: 67.91428571428571\n",
      "Epoch: [7/8], Step: [1001/4300],Train Acc:72.23953488372094, Validation Acc: 68.45714285714286\n",
      "Epoch: [7/8], Step: [2001/4300],Train Acc:72.38604651162791, Validation Acc: 68.47619047619048\n",
      "Epoch: [7/8], Step: [3001/4300],Train Acc:72.24418604651163, Validation Acc: 68.32380952380953\n",
      "Epoch: [7/8], Step: [4001/4300],Train Acc:72.48372093023256, Validation Acc: 68.21904761904761\n",
      "Epoch: [8/8], Step: [1001/4300],Train Acc:72.56279069767442, Validation Acc: 67.93333333333334\n",
      "Epoch: [8/8], Step: [2001/4300],Train Acc:72.35813953488372, Validation Acc: 67.35238095238095\n",
      "Epoch: [8/8], Step: [3001/4300],Train Acc:72.52093023255814, Validation Acc: 69.33333333333333\n",
      "Epoch: [8/8], Step: [4001/4300],Train Acc:72.62558139534883, Validation Acc: 67.6952380952381\n"
     ]
    }
   ],
   "source": [
    "#Final Model\n",
    "num_epochs = 8# number epoch to train\n",
    "learning_rates = .0001\n",
    "emb_dim = 61\n",
    "\n",
    "loss_performance = []\n",
    "acc_performance = []\n",
    "\n",
    "model = Net_dropout(emb_dim)\n",
    "    \n",
    "criterion = torch.nn.BCEWithLogitsLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rates)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=.1)\n",
    "    \n",
    "loss_vals, train_est, acc_est = model_train(emb_dim = emb_dim, model = model, learning_rate = learning_rates, \\\n",
    "                                 num_epochs = num_epochs, criterion = criterion, optimizer = optimizer, \\\n",
    "                                 train_loader = trainloader, val_loader = valloader)\n",
    "    \n",
    "loss_performance.append(loss_vals)\n",
    "acc_performance.append(test_model(trainloader, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), models + \"model.pt\")\n",
    "#pickle.dump(model, open(local_pickles + \"model.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualified Crowds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN for High Pol knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol_knowledge_df_orig = pd.read_pickle(data_pickles + 'pol_knowledge_df_train_orig.p')\n",
    "pol_knowledge_df_val = pd.read_pickle(data_pickles + 'pol_knowledge_df_val.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the data\n",
    "X_data_pol_train = feature_creation(pol_knowledge_df_orig)\n",
    "X_data_pol_val = feature_creation(pol_knowledge_df_val)\n",
    "\n",
    "\n",
    "# Preparing the data\n",
    "#X_data = feature_creation(train_data_fakesource.iloc[:,:])\n",
    "#Y_data = train_target_large_lowcred[:]\n",
    "\n",
    "y_data_bin = np.where(Y_data == True, 1, 0)\n",
    "\n",
    "test = StandardScaler()\n",
    "test.fit(X_data_pol_train[numeric_features])\n",
    "\n",
    "#training data\n",
    "X_train_stand = X_data_pol_train.copy()\n",
    "X_train_stand[numeric_features] = test.transform(X_train_stand[numeric_features])\n",
    "\n",
    "#test data\n",
    "X_val_stand = X_data_pol_val.copy()\n",
    "X_val_stand[numeric_features] = test.transform(X_val_stand[numeric_features])\n",
    "\n",
    "#test data\n",
    "#X_test_stand = X_data_pol_test.copy()\n",
    "#X_test_stand[numeric_features] = test.transform(X_test_stand[numeric_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_prep(X_train_stand,Y_data)\n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=10,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "val_data = data_prep(X_val_stand,Y_data_val)\n",
    "valloader = torch.utils.data.DataLoader(val_data, batch_size=10,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "#test_data = data_loader(X_test_stand,test_target_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/15], Step: [1001/4300],Train Acc:68.37906976744186, Validation Acc: 61.61904761904762\n",
      "Epoch: [1/15], Step: [2001/4300],Train Acc:69.42558139534884, Validation Acc: 62.12380952380953\n",
      "Epoch: [1/15], Step: [3001/4300],Train Acc:70.50232558139535, Validation Acc: 63.20952380952381\n",
      "Epoch: [1/15], Step: [4001/4300],Train Acc:72.30465116279069, Validation Acc: 64.8\n",
      "Epoch: [2/15], Step: [1001/4300],Train Acc:72.6, Validation Acc: 65.46666666666667\n",
      "Epoch: [2/15], Step: [2001/4300],Train Acc:73.37906976744186, Validation Acc: 66.67619047619047\n",
      "Epoch: [2/15], Step: [3001/4300],Train Acc:73.32790697674419, Validation Acc: 68.47619047619048\n",
      "Epoch: [2/15], Step: [4001/4300],Train Acc:73.19534883720931, Validation Acc: 68.82857142857142\n",
      "Epoch: [3/15], Step: [1001/4300],Train Acc:73.8953488372093, Validation Acc: 67.28571428571429\n",
      "Epoch: [3/15], Step: [2001/4300],Train Acc:74.05813953488372, Validation Acc: 67.64761904761905\n",
      "Epoch: [3/15], Step: [3001/4300],Train Acc:74.02790697674419, Validation Acc: 67.77142857142857\n",
      "Epoch: [3/15], Step: [4001/4300],Train Acc:74.5046511627907, Validation Acc: 69.0\n",
      "Epoch: [4/15], Step: [1001/4300],Train Acc:74.54651162790698, Validation Acc: 69.12380952380953\n",
      "Epoch: [4/15], Step: [2001/4300],Train Acc:74.67209302325581, Validation Acc: 68.4\n",
      "Epoch: [4/15], Step: [3001/4300],Train Acc:74.56976744186046, Validation Acc: 68.76190476190476\n",
      "Epoch: [4/15], Step: [4001/4300],Train Acc:74.62790697674419, Validation Acc: 68.83809523809524\n",
      "Epoch: [5/15], Step: [1001/4300],Train Acc:74.57209302325582, Validation Acc: 68.08571428571429\n",
      "Epoch: [5/15], Step: [2001/4300],Train Acc:74.81860465116279, Validation Acc: 69.13333333333334\n",
      "Epoch: [5/15], Step: [3001/4300],Train Acc:74.84418604651162, Validation Acc: 69.08571428571429\n",
      "Epoch: [5/15], Step: [4001/4300],Train Acc:74.86279069767441, Validation Acc: 68.21904761904761\n",
      "Epoch: [6/15], Step: [1001/4300],Train Acc:75.13255813953488, Validation Acc: 68.18095238095238\n",
      "Epoch: [6/15], Step: [2001/4300],Train Acc:75.06744186046511, Validation Acc: 69.00952380952381\n",
      "Epoch: [6/15], Step: [3001/4300],Train Acc:75.29767441860466, Validation Acc: 69.19047619047619\n",
      "Epoch: [6/15], Step: [4001/4300],Train Acc:75.35813953488372, Validation Acc: 68.61904761904762\n",
      "Epoch: [7/15], Step: [1001/4300],Train Acc:75.34418604651162, Validation Acc: 69.03809523809524\n",
      "Epoch: [7/15], Step: [2001/4300],Train Acc:75.39302325581396, Validation Acc: 68.64761904761905\n",
      "Epoch: [7/15], Step: [3001/4300],Train Acc:75.35116279069767, Validation Acc: 69.08571428571429\n",
      "Epoch: [7/15], Step: [4001/4300],Train Acc:75.20232558139534, Validation Acc: 67.57142857142857\n",
      "Epoch: [8/15], Step: [1001/4300],Train Acc:75.34883720930233, Validation Acc: 69.46666666666667\n",
      "Epoch: [8/15], Step: [2001/4300],Train Acc:75.59767441860465, Validation Acc: 67.98095238095237\n",
      "Epoch: [8/15], Step: [3001/4300],Train Acc:75.6046511627907, Validation Acc: 68.18095238095238\n",
      "Epoch: [8/15], Step: [4001/4300],Train Acc:75.84883720930233, Validation Acc: 69.33333333333333\n",
      "Epoch: [9/15], Step: [1001/4300],Train Acc:76.14651162790697, Validation Acc: 69.24761904761905\n",
      "Epoch: [9/15], Step: [2001/4300],Train Acc:74.95348837209302, Validation Acc: 66.52380952380952\n",
      "Epoch: [9/15], Step: [3001/4300],Train Acc:76.1767441860465, Validation Acc: 68.72380952380952\n",
      "Epoch: [9/15], Step: [4001/4300],Train Acc:75.93023255813954, Validation Acc: 68.82857142857142\n",
      "Epoch: [10/15], Step: [1001/4300],Train Acc:76.35348837209303, Validation Acc: 68.82857142857142\n",
      "Epoch: [10/15], Step: [2001/4300],Train Acc:75.92093023255813, Validation Acc: 68.45714285714286\n",
      "Epoch: [10/15], Step: [3001/4300],Train Acc:76.55116279069767, Validation Acc: 68.82857142857142\n",
      "Epoch: [10/15], Step: [4001/4300],Train Acc:76.53488372093024, Validation Acc: 69.53333333333333\n",
      "Epoch: [11/15], Step: [1001/4300],Train Acc:76.63953488372093, Validation Acc: 69.38095238095238\n",
      "Epoch: [11/15], Step: [2001/4300],Train Acc:76.58372093023256, Validation Acc: 68.17142857142858\n",
      "Epoch: [11/15], Step: [3001/4300],Train Acc:76.56279069767442, Validation Acc: 66.6\n",
      "Epoch: [11/15], Step: [4001/4300],Train Acc:76.73488372093023, Validation Acc: 68.97142857142858\n",
      "Epoch: [12/15], Step: [1001/4300],Train Acc:77.24883720930232, Validation Acc: 68.91428571428571\n",
      "Epoch: [12/15], Step: [2001/4300],Train Acc:76.89302325581396, Validation Acc: 68.98095238095237\n",
      "Epoch: [12/15], Step: [3001/4300],Train Acc:77.39302325581396, Validation Acc: 68.26666666666667\n",
      "Epoch: [12/15], Step: [4001/4300],Train Acc:76.9953488372093, Validation Acc: 69.14285714285714\n",
      "Epoch: [13/15], Step: [1001/4300],Train Acc:77.42558139534884, Validation Acc: 68.22857142857143\n",
      "Epoch: [13/15], Step: [2001/4300],Train Acc:77.80232558139535, Validation Acc: 68.07619047619048\n",
      "Epoch: [13/15], Step: [3001/4300],Train Acc:77.31860465116279, Validation Acc: 67.44761904761904\n",
      "Epoch: [13/15], Step: [4001/4300],Train Acc:77.62093023255814, Validation Acc: 68.28571428571429\n",
      "Epoch: [14/15], Step: [1001/4300],Train Acc:78.02093023255814, Validation Acc: 68.42857142857143\n",
      "Epoch: [14/15], Step: [2001/4300],Train Acc:78.21162790697674, Validation Acc: 68.5904761904762\n",
      "Epoch: [14/15], Step: [3001/4300],Train Acc:78.12325581395349, Validation Acc: 69.36190476190477\n",
      "Epoch: [14/15], Step: [4001/4300],Train Acc:78.6046511627907, Validation Acc: 69.05714285714286\n",
      "Epoch: [15/15], Step: [1001/4300],Train Acc:78.64186046511628, Validation Acc: 67.60952380952381\n",
      "Epoch: [15/15], Step: [2001/4300],Train Acc:77.83488372093024, Validation Acc: 69.08571428571429\n",
      "Epoch: [15/15], Step: [3001/4300],Train Acc:78.46511627906976, Validation Acc: 68.4952380952381\n",
      "Epoch: [15/15], Step: [4001/4300],Train Acc:78.66511627906976, Validation Acc: 68.98095238095237\n"
     ]
    }
   ],
   "source": [
    "#Final Model\n",
    "num_epochs = 15# number epoch to train\n",
    "learning_rates = .0001\n",
    "emb_dim = 61\n",
    "\n",
    "loss_performance = []\n",
    "acc_performance = []\n",
    "\n",
    "model = Net(emb_dim)\n",
    "    \n",
    "criterion = torch.nn.BCEWithLogitsLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rates)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=.1)\n",
    "    \n",
    "loss_vals, train_est, acc_est = model_train(emb_dim = emb_dim, model = model, learning_rate = learning_rates, \\\n",
    "                                 num_epochs = num_epochs, criterion = criterion, optimizer = optimizer, \\\n",
    "                                 train_loader = trainloader, val_loader = valloader)\n",
    "    \n",
    "loss_performance.append(loss_vals)\n",
    "acc_performance.append(test_model(trainloader, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), models + \"model_nn_highpol.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest high political knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pol_knowledge_df_all = X_data_pol_train.append(X_data_pol_val)\n",
    "\n",
    "pol_knowledge_df_all.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0])),\n",
       "             error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('preprocessor',\n",
       "                                        ColumnTransformer(n_jobs=None,\n",
       "                                                          remainder='drop',\n",
       "                                                          sparse_threshold=0.3,\n",
       "                                                          transformer_weights=None,\n",
       "                                                          transformers=[('num',\n",
       "                                                                         Pipeline(memory=None,\n",
       "                                                                                  steps=[('scaler',\n",
       "                                                                                          StandardScaler(copy=True,\n",
       "                                                                                                         with_mean=True,\n",
       "                                                                                                         with_s...\n",
       "                                                               random_state=None,\n",
       "                                                               verbose=0,\n",
       "                                                               warm_start=False))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'classifier__max_depth': [1, 2, 3, 4, 5, 10, 20, None],\n",
       "                         'classifier__min_samples_leaf': [1, 2, 3, 5, 10],\n",
       "                         'classifier__min_samples_split': [2, 3, 5, 10],\n",
       "                         'classifier__n_estimators': [500, 1000, 2000]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[('num', numeric_transformer, numeric_features)])\n",
    "\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', RandomForestClassifier())])\n",
    "\n",
    "parameters = {'classifier__max_depth':[1,2,3,4,5,10,20,None], \\\n",
    "               'classifier__min_samples_split':[2,3,5,10], \\\n",
    "               'classifier__min_samples_leaf':[1,2,3,5,10], \\\n",
    "               'classifier__n_estimators':[500, 1000, 2000]}\n",
    "\n",
    "grid_rf_pol = GridSearchCV(clf, scoring = 'accuracy', param_grid=parameters, cv=ps)\n",
    "\n",
    "grid_rf_pol.fit(pol_knowledge_df_all,Y_data_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(grid_rf_pol, open(models + \"grid_rf_pol.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(grid_rf.predict(X_data_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
