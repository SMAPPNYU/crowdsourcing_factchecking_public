{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author of code: William Godel\n",
    "\n",
    "Date: 07/02\n",
    "\n",
    "Purpose: to test performance of models on alternative data distributions\n",
    "\n",
    "## Data IN: \n",
    "\n",
    "test_data_98.csv\n",
    "\n",
    "test_data_90.csv\n",
    "\n",
    "test_data_75.csv\n",
    "\n",
    "test_data_10.csv\n",
    "\n",
    "\n",
    "test_data_98_highpol.csv\n",
    "\n",
    "test_data_90_highpol.csv\n",
    "\n",
    "test_data_75_highpol.csv\n",
    "\n",
    "test_data_10_highpol.csv\n",
    "\n",
    "grid_rf.p\n",
    "\n",
    "model_nn_highpol.pt\n",
    "\n",
    "model_large.pt\n",
    "\n",
    "## Data OUT:\n",
    "\n",
    "no data out, just statistics\n",
    "\n",
    "\n",
    "Machine: My laptop or Imac\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "laptop = True\n",
    "import pickle\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "from functions import count_mode, bayes_probs, bayes_binary\n",
    "from ml_functions import feature_creation\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#crowd_size, feature_transform\n",
    "\n",
    "from path import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_98 = pd.read_csv(prepared_data + \"test_data_98.csv\")\n",
    "test_data_90 = pd.read_csv(prepared_data + \"test_data_90.csv\")\n",
    "test_data_75 = pd.read_csv(prepared_data + \"test_data_75.csv\")\n",
    "test_data_10 = pd.read_csv(prepared_data + \"test_data_10.csv\")\n",
    "\n",
    "test_data_98_target = test_data_98['mode'] == 'FM'\n",
    "test_data_90_target = test_data_90['mode'] == 'FM'\n",
    "test_data_75_target = test_data_75['mode'] == 'FM'\n",
    "test_data_10_target = test_data_10['mode'] == 'FM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_data_98 = feature_creation(test_data_98)\n",
    "X_test_data_90 = feature_creation(test_data_90)\n",
    "X_test_data_75 = feature_creation(test_data_75)\n",
    "X_test_data_10 = feature_creation(test_data_10)\n",
    "\n",
    "X_test_data_98_large = feature_creation(test_data_98, crowd_num = 25)\n",
    "X_test_data_90_large = feature_creation(test_data_90, crowd_num = 25)\n",
    "X_test_data_75_large = feature_creation(test_data_75, crowd_num = 25)\n",
    "X_test_data_10_large = feature_creation(test_data_10, crowd_num = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = []\n",
    "\n",
    "resp_var = [x for x in X_test_data_98.columns if 'resp_veracity_' in x]\n",
    "new_cols = [x for x in X_test_data_98.columns if 'new' in x]\n",
    "\n",
    "#numeric_features.extend(resp_var)\n",
    "numeric_features.extend(resp_var) #rempve this if it doesn't work\n",
    "numeric_features.extend(new_cols)\n",
    "numeric_features.extend([\"crowd_means\",'crowd_median','crowd_full_range', 'crowd_IQR_range', \\\n",
    "                         'crowd_variance', 'crowd_bayes'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_eval(preds, truth):\n",
    "    \n",
    "    if True in np.unique(truth):\n",
    "        \n",
    "        condition_pos = True\n",
    "        condition_neg = False\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        condition_pos = 1\n",
    "        condition_neg = 0\n",
    "        \n",
    "    con_pos_count = np.sum(truth == condition_pos)\n",
    "    con_neg_count = np.sum(truth == condition_neg)\n",
    "        \n",
    "    accuracy = np.sum((preds == truth))/truth.size\n",
    "    \n",
    "    True_pos = np.sum(preds[truth == condition_pos] == truth[truth ==condition_pos])\n",
    "    False_pos = np.sum(preds[truth == condition_pos] != truth[truth ==condition_pos])\n",
    "    \n",
    "    True_neg = np.sum(preds[truth == condition_neg] == truth[truth == condition_neg])\n",
    "    False_neg = np.sum(preds[truth == condition_neg] != truth[truth == condition_neg])\n",
    "    \n",
    "    \n",
    "    return accuracy, True_pos, False_pos, True_neg, False_neg\n",
    "    \n",
    "    \n",
    "def conf_perc(a_list):\n",
    "    \n",
    "    total_pos = sum(a_list[2:4])\n",
    "    total_neg = sum(a_list[4:])\n",
    "    print(\"TP\", \"FP\", \"TN\", \"FN\")\n",
    "    return a_list[2]/total_pos, a_list[3]/total_pos, a_list[4]/total_neg, a_list[5]/total_neg\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "### standard crowd\n",
    "### size 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_dic_98 = {}\n",
    "performance_dic_90 = {}\n",
    "performance_dic_75 = {}\n",
    "performance_dic_10 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_rf = pickle.load(open(models + 'grid_rf.p', \"rb\" ))\n",
    "\n",
    "algo = grid_rf\n",
    "\n",
    "algo_name = \"randomforest_10\"\n",
    "\n",
    "all_results = [algo_name]\n",
    "results = list(conf_eval(algo.predict(X_test_data_98), test_data_98_target))\n",
    "all_results.extend(results)\n",
    "performance_dic_98[algo_name] = all_results\n",
    "\n",
    "all_results = [algo_name]\n",
    "results = list(conf_eval(algo.predict(X_test_data_90), test_data_90_target))\n",
    "all_results.extend(results)\n",
    "performance_dic_90[algo_name] = all_results\n",
    "\n",
    "all_results = [algo_name]\n",
    "results = list(conf_eval(algo.predict(X_test_data_75), test_data_75_target))\n",
    "all_results.extend(results)\n",
    "performance_dic_75[algo_name] = all_results\n",
    "\n",
    "results = [algo_name]\n",
    "all_results = list(conf_eval(algo.predict(X_test_data_10), test_data_10_target))\n",
    "all_results.extend(results)\n",
    "performance_dic_10[algo_name] = all_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP FP TN FN\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.48947368421052634,\n",
       " 0.5105263157894737,\n",
       " 0.6969924812030075,\n",
       " 0.3030075187969925)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_perc(performance_dic_98['randomforest_10'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['randomforest_10', 0.6928421052631579, 93, 97, 6489, 2821]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance_dic_98['randomforest_10']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualified Crowds\n",
    "\n",
    "### loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_98_highpk = pd.read_csv(prepared_data + 'test_data_98_highpol.csv')\n",
    "test_data_90_highpk = pd.read_csv(prepared_data + \"test_data_90_highpol.csv\")\n",
    "test_data_75_highpk = pd.read_csv(prepared_data + \"test_data_75_highpol.csv\")\n",
    "test_data_10_highpk = pd.read_csv(prepared_data + \"test_data_10_highpol.csv\")\n",
    "\n",
    "\n",
    "X_test_data_98_highpk = feature_creation(test_data_98_highpk)\n",
    "X_test_data_90_highpk = feature_creation(test_data_90_highpk)\n",
    "X_test_data_75_highpk = feature_creation(test_data_75_highpk)\n",
    "X_test_data_10_highpk = feature_creation(test_data_10_highpk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dim):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(emb_dim, 500) \n",
    "        self.fc2 = nn.Linear(500, 100)\n",
    "        self.fc3 = nn.Linear(100, 25)\n",
    "        self.fc4 = nn.Linear(25, 25)\n",
    "        self.fc5 = nn.Linear(25, 10)\n",
    "        self.fc6 = nn.Linear(10, 1)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class data_loader(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list.values\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        data = self.data_list[key]\n",
    "        label = self.target_list[key]\n",
    "        return [data, label]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "emb_dim = 61   \n",
    "net_large = Net(emb_dim)\n",
    "net_large.load_state_dict(torch.load(models + \"model_nn_highpol.pt\"))\n",
    "net_large.eval()\n",
    "\n",
    "# Preparing the data\n",
    "y_data_bin_large = np.where(test_data_98_target == True, 1, 0)\n",
    "data = X_test_data_98_highpk\n",
    "nn_list_large = [\"nn_10_pk\"]\n",
    "test = StandardScaler()\n",
    "test.fit(data[numeric_features])\n",
    "#training data\n",
    "X_train_stand_large = data.copy()\n",
    "X_train_stand_large[numeric_features] = test.transform(data[numeric_features])\n",
    "train_dat_nn_large = data_loader(X_train_stand_large,y_data_bin_large)\n",
    "nn_preds  = torch.sigmoid(net_large(torch.tensor(train_dat_nn_large[:][0]).float()))\n",
    "nn_preds = np.where(nn_preds > .5, 1,0).reshape(-1,)\n",
    "all_results = list(conf_eval(nn_preds, y_data_bin_large))\n",
    "\n",
    "nn_list_large.extend(all_results)\n",
    "performance_dic_98['nn_10_pk'] = nn_list_large\n",
    "\n",
    "\n",
    "\n",
    "# Preparing the data\n",
    "y_data_bin_large = np.where(test_data_90_target == True, 1, 0)\n",
    "data = X_test_data_90_highpk\n",
    "nn_list_large = [\"nn_25\"]\n",
    "test = StandardScaler()\n",
    "test.fit(data[numeric_features])\n",
    "#training data\n",
    "X_train_stand_large = data.copy()\n",
    "X_train_stand_large[numeric_features] = test.transform(data[numeric_features])\n",
    "train_dat_nn_large = data_loader(X_train_stand_large,y_data_bin_large)\n",
    "nn_preds  = torch.sigmoid(net_large(torch.tensor(train_dat_nn_large[:][0]).float()))\n",
    "nn_preds = np.where(nn_preds > .5, 1,0).reshape(-1,)\n",
    "all_results = list(conf_eval(nn_preds, y_data_bin_large))\n",
    "\n",
    "nn_list_large.extend(all_results)\n",
    "performance_dic_90['nn_10_pk'] = nn_list_large\n",
    "\n",
    "\n",
    "# Preparing the data\n",
    "y_data_bin_large = np.where(test_data_75_target == True, 1, 0)\n",
    "data = X_test_data_75_highpk\n",
    "nn_list_large = [\"nn_10_pk\"]\n",
    "test = StandardScaler()\n",
    "test.fit(data[numeric_features])\n",
    "#training data\n",
    "X_train_stand_large = data.copy()\n",
    "X_train_stand_large[numeric_features] = test.transform(data[numeric_features])\n",
    "train_dat_nn_large = data_loader(X_train_stand_large,y_data_bin_large)\n",
    "nn_preds  = torch.sigmoid(net_large(torch.tensor(train_dat_nn_large[:][0]).float()))\n",
    "nn_preds = np.where(nn_preds > .5, 1,0).reshape(-1,)\n",
    "all_results = list(conf_eval(nn_preds, y_data_bin_large))\n",
    "\n",
    "nn_list_large.extend(all_results)\n",
    "performance_dic_75['nn_10_pk'] = nn_list_large\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Preparing the data\n",
    "y_data_bin_large = np.where(test_data_10_target == True, 1, 0)\n",
    "data = X_test_data_10_highpk\n",
    "nn_list_large = [\"nn_10_pk\"]\n",
    "test = StandardScaler()\n",
    "test.fit(data[numeric_features])\n",
    "#training data\n",
    "X_train_stand_large = data.copy()\n",
    "X_train_stand_large[numeric_features] = test.transform(data[numeric_features])\n",
    "train_dat_nn_large = data_loader(X_train_stand_large,y_data_bin_large)\n",
    "nn_preds  = torch.sigmoid(net_large(torch.tensor(train_dat_nn_large[:][0]).float()))\n",
    "nn_preds = np.where(nn_preds > .5, 1,0).reshape(-1,)\n",
    "all_results = list(conf_eval(nn_preds, y_data_bin_large))\n",
    "\n",
    "nn_list_large.extend(all_results)\n",
    "performance_dic_10['nn_10_pk'] = nn_list_large\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nn_10_pk', 0.5401052631578948, 4347, 4203, 784, 166]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance_dic_10['nn_10_pk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'randomforest_10': ['randomforest_10',\n",
       "  0.6980734814190968,\n",
       "  442,\n",
       "  507,\n",
       "  6189,\n",
       "  2361],\n",
       " 'nn_10_pk': ['nn_25', 0.7070217917675545, 587, 362, 6129, 2421]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance_dic_90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network\n",
    "### Large crowd\n",
    "### size 25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features_large = []\n",
    "\n",
    "resp_var = [x for x in X_test_data_98_large.columns if 'resp_veracity_' in x]\n",
    "new_cols = [x for x in X_test_data_98_large.columns if 'new' in x]\n",
    "\n",
    "#numeric_features.extend(resp_var)\n",
    "numeric_features_large.extend(resp_var) #rempve this if it doesn't work\n",
    "numeric_features_large.extend(new_cols)\n",
    "numeric_features_large.extend([\"crowd_means\",'crowd_median','crowd_full_range', 'crowd_IQR_range', \\\n",
    "                         'crowd_variance', 'crowd_bayes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "emb_dim = 121   \n",
    "net_large = Net(emb_dim)\n",
    "net_large.load_state_dict(torch.load(models + \"model_large.pt\"))\n",
    "net_large.eval()\n",
    "\n",
    "\n",
    "\n",
    "# Preparing the data\n",
    "y_data_bin_large = np.where(test_data_98_target == True, 1, 0)\n",
    "data = X_test_data_98_large\n",
    "nn_list_large = [\"nn_25\"]\n",
    "test = StandardScaler()\n",
    "test.fit(data[numeric_features_large])\n",
    "#training data\n",
    "X_train_stand_large = data.copy()\n",
    "X_train_stand_large[numeric_features_large] = test.transform(data[numeric_features_large])\n",
    "train_dat_nn_large = data_loader(X_train_stand_large,y_data_bin_large)\n",
    "nn_preds  = torch.sigmoid(net_large(torch.tensor(train_dat_nn_large[:][0]).float()))\n",
    "nn_preds = np.where(nn_preds > .5, 1,0).reshape(-1,)\n",
    "all_results = list(conf_eval(nn_preds, y_data_bin_large))\n",
    "\n",
    "nn_list_large.extend(all_results)\n",
    "performance_dic_98['nn_25'] = nn_list_large\n",
    "\n",
    "\n",
    "\n",
    "# Preparing the data\n",
    "y_data_bin_large = np.where(test_data_90_target == True, 1, 0)\n",
    "data = X_test_data_90_large\n",
    "nn_list_large = [\"nn_25\"]\n",
    "test = StandardScaler()\n",
    "test.fit(data[numeric_features_large])\n",
    "#training data\n",
    "X_train_stand_large = data.copy()\n",
    "X_train_stand_large[numeric_features_large] = test.transform(data[numeric_features_large])\n",
    "train_dat_nn_large = data_loader(X_train_stand_large,y_data_bin_large)\n",
    "nn_preds  = torch.sigmoid(net_large(torch.tensor(train_dat_nn_large[:][0]).float()))\n",
    "nn_preds = np.where(nn_preds > .5, 1,0).reshape(-1,)\n",
    "all_results = list(conf_eval(nn_preds, y_data_bin_large))\n",
    "\n",
    "nn_list_large.extend(all_results)\n",
    "performance_dic_90['nn_25'] = nn_list_large\n",
    "\n",
    "\n",
    "\n",
    "# Preparing the data\n",
    "y_data_bin_large = np.where(test_data_75_target == True, 1, 0)\n",
    "data = X_test_data_75_large\n",
    "nn_list_large = [\"nn_25\"]\n",
    "test = StandardScaler()\n",
    "test.fit(data[numeric_features_large])\n",
    "#training data\n",
    "X_train_stand_large = data.copy()\n",
    "X_train_stand_large[numeric_features_large] = test.transform(data[numeric_features_large])\n",
    "train_dat_nn_large = data_loader(X_train_stand_large,y_data_bin_large)\n",
    "nn_preds  = torch.sigmoid(net_large(torch.tensor(train_dat_nn_large[:][0]).float()))\n",
    "nn_preds = np.where(nn_preds > .5, 1,0).reshape(-1,)\n",
    "all_results = list(conf_eval(nn_preds, y_data_bin_large))\n",
    "\n",
    "nn_list_large.extend(all_results)\n",
    "performance_dic_75['nn_25'] = nn_list_large\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Preparing the data\n",
    "y_data_bin_large = np.where(test_data_10_target == True, 1, 0)\n",
    "data = X_test_data_10_large\n",
    "nn_list_large = [\"nn_25\"]\n",
    "test = StandardScaler()\n",
    "test.fit(data[numeric_features_large])\n",
    "#training data\n",
    "X_train_stand_large = data.copy()\n",
    "X_train_stand_large[numeric_features_large] = test.transform(data[numeric_features_large])\n",
    "train_dat_nn_large = data_loader(X_train_stand_large,y_data_bin_large)\n",
    "nn_preds  = torch.sigmoid(net_large(torch.tensor(train_dat_nn_large[:][0]).float()))\n",
    "nn_preds = np.where(nn_preds > .5, 1,0).reshape(-1,)\n",
    "all_results = list(conf_eval(nn_preds, y_data_bin_large))\n",
    "\n",
    "nn_list_large.extend(all_results)\n",
    "performance_dic_10['nn_25'] = nn_list_large\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nn_25', 0.49757894736842107, 3908, 4642, 819, 131]\n",
      "TP FP TN FN\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.45707602339181286,\n",
       " 0.5429239766081871,\n",
       " 0.8621052631578947,\n",
       " 0.13789473684210526)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(performance_dic_10['nn_25'])\n",
    "conf_perc(performance_dic_10['nn_25'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nn_25', 0.7170526315789474, 1294, 1081, 5518, 1607]\n",
      "TP FP TN FN\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5448421052631579,\n",
       " 0.4551578947368421,\n",
       " 0.7744561403508772,\n",
       " 0.2255438596491228)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(performance_dic_75['nn_25'])\n",
    "conf_perc(performance_dic_75['nn_25'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nn_25', 0.7379724181492788, 553, 396, 6457, 2093]\n",
      "TP FP TN FN\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5827186512118019,\n",
       " 0.4172813487881981,\n",
       " 0.7552046783625731,\n",
       " 0.2447953216374269)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(performance_dic_90['nn_25'])\n",
    "conf_perc(performance_dic_90['nn_25'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nn_25', 0.738421052631579, 109, 81, 6906, 2404]\n",
      "TP FP TN FN\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5736842105263158,\n",
       " 0.4263157894736842,\n",
       " 0.7417830290010741,\n",
       " 0.2582169709989259)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(performance_dic_98['nn_25'])\n",
    "conf_perc(performance_dic_98['nn_25'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nn_10_pk', 0.5401052631578948, 4347, 4203, 784, 166]\n",
      "TP FP TN FN\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.508421052631579,\n",
       " 0.49157894736842106,\n",
       " 0.8252631578947368,\n",
       " 0.17473684210526316)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(performance_dic_10['nn_10_pk'])\n",
    "conf_perc(performance_dic_10['nn_10_pk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nn_10_pk', 0.7055789473684211, 1430, 945, 5273, 1852]\n",
      "TP FP TN FN\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6021052631578947,\n",
       " 0.39789473684210525,\n",
       " 0.7400701754385965,\n",
       " 0.2599298245614035)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(performance_dic_75['nn_10_pk'])\n",
    "conf_perc(performance_dic_75['nn_10_pk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nn_25', 0.7070217917675545, 587, 362, 6129, 2421]\n",
      "TP FP TN FN\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6185458377239199, 0.3814541622760801, 0.716842105263158, 0.2831578947368421)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(performance_dic_90['nn_10_pk'])\n",
    "conf_perc(performance_dic_90['nn_10_pk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nn_10_pk', 0.7001052631578948, 116, 74, 6535, 2775]\n",
      "TP FP TN FN\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6105263157894737,\n",
       " 0.3894736842105263,\n",
       " 0.7019334049409237,\n",
       " 0.2980665950590763)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(performance_dic_98['nn_10_pk'])\n",
    "conf_perc(performance_dic_98['nn_10_pk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
