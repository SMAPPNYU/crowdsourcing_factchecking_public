{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author of code: William Godel \n",
    "\n",
    "Date: 07/02\n",
    "\n",
    "Purpose: to convert additional noncovid original data into crowds\n",
    "\n",
    "## Data IN: \n",
    "\n",
    "non_Covid_fact_checkers_summary_clean.csv\n",
    "non_Covid_fact_checkers_clean.csv\n",
    "full_survey_noncovid_control_clean.csv\n",
    "\n",
    "## Data OUT: \n",
    "\n",
    "### Data tables\n",
    "\n",
    "train_data_large_noncovid.csv\n",
    "\n",
    "val_data_large_noncovid.csv\n",
    "\n",
    "test_data_large_noncovid.csv\n",
    "\n",
    "\n",
    "### dictionaries\n",
    "\n",
    "article_all_noncovid.p\n",
    "\n",
    "article_dic_noncovid.p\n",
    "\n",
    "response_dic_noncovid.p\n",
    "\n",
    "### Sets \n",
    "\n",
    "test_set_noncovid.p\n",
    "\n",
    "val_set_noncovid.p\n",
    "\n",
    "train_set_noncovid.p\n",
    "\n",
    "\n",
    "Machine: My laptop or Imac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "import copy\n",
    "from collections import Counter\n",
    "from scipy.stats import multinomial\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "\n",
    "##adjust your master directory below\n",
    "from path import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = pd.read_csv(source_data + \"non_Covid_fact_checkers_summary_clean.csv\", index_col = 0) \n",
    "answers_individual = pd.read_csv(source_data + \"non_Covid_fact_checkers_clean.csv\", index_col = 0) \n",
    "control_data = pd.read_csv(source_data + \"full_survey_noncovid_control_clean.csv\", index_col = 0, low_memory=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rel_likert = ['clean_True_Likert_Scale_1',\n",
    "             'clean_True_Likert_Scale_2',\n",
    "             'clean_True_Likert_Scale_3',\n",
    "             'clean_True_Likert_Scale_4',\n",
    "             'clean_True_Likert_Scale_5']\n",
    "\n",
    "fact_likert = ['Likert_1_mean',\n",
    "              'Likert_2_mean',\n",
    "              'Likert_3_mean',\n",
    "              'Likert_4_mean',\n",
    "              'Likert_5_mean']\n",
    "\n",
    "#just getting the answer\n",
    "for this_col in rel_likert:\n",
    "    \n",
    "    control_data[this_col] = control_data[this_col].str[0]\n",
    "\n",
    "#not - asked as nan\n",
    "for this_col in rel_likert:\n",
    "    \n",
    "    control_data.loc[control_data[this_col] == 'n', this_col] = np.nan\n",
    "    \n",
    "    control_data[this_col] = control_data[this_col].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identifying if users were correct or not based on model fact checker\n",
    "#delete if not used later\n",
    "\n",
    "\n",
    "control_data.reset_index(inplace = True, drop = True)\n",
    "\n",
    "eval_cols = ['clean_Q13_Eval_1',\n",
    "             'clean_Q13_Eval_2',\n",
    "             'clean_Q13_Eval_3',\n",
    "             'clean_Q13_Eval_4',\n",
    "             'clean_Q13_Eval_5']\n",
    "\n",
    "\n",
    "vote_list = ['Eval_1_mode',\n",
    "             'Eval_2_mode',\n",
    "             'Eval_3_mode',\n",
    "             'Eval_4_mode',\n",
    "             'Eval_5_mode']\n",
    "\n",
    "correct_cols = ['correct_1',\n",
    "               'correct_2',\n",
    "               'correct_3',\n",
    "               'correct_4',\n",
    "               'correct_5']\n",
    "\n",
    "control_data['correct_1'] = np.nan\n",
    "control_data['correct_2'] = np.nan\n",
    "control_data['correct_3'] = np.nan\n",
    "control_data['correct_4'] = np.nan\n",
    "control_data['correct_5'] = np.nan\n",
    "\n",
    "answer_dic = {'CND':'Determine', 'T':'factually accurate', 'FM':'Misleading'}\n",
    "\n",
    "for user in range(control_data.shape[0]):\n",
    "    \n",
    "    for judge_num, judge_col in enumerate(eval_cols):\n",
    "        \n",
    "        if control_data.loc[user,judge_col] == 'not asked': #if the user didn't answer -> skip it\n",
    "            continue\n",
    "        elif control_data.loc[user,vote_list[judge_num]]  == 'No Mode!': #if fact checkers don't agree -> skip it\n",
    "            continue\n",
    "        else:\n",
    "            \n",
    "            fact_answer = control_data.loc[user,vote_list[judge_num]]\n",
    "            user_answer_iftrue = answer_dic[fact_answer]\n",
    "            \n",
    "            if user_answer_iftrue in control_data.loc[user,judge_col]:\n",
    "                \n",
    "                control_data.loc[user,correct_cols[judge_num]] = 1\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                control_data.loc[user,correct_cols[judge_num]] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_data['pol_knowledge'] = control_data['Q_PK_1'].isin([\"Democratic Party\"])*1 + control_data['Q_PK_3'].isin([\"Nancy Pelosi\"])*1 + control_data['Q_PK_6'].isin([\"Prime Minister of the United Kingdom\"])*1 + control_data['Q_PK_7'].isin([\"Michael Pompeo\"])*1\n",
    "\n",
    "#control_data['tot_crt'] = control_data['Q_1st_CRT'].str.contains(\"2\", regex = False, case = False)*1 + control_data['Q_2nd_CRT'].isin([\"7\"])*1 + control_data['Q_3rd_CRT'].str.contains(\"emily\", regex = False, case = False)*1 + control_data['Q_4th_CRT'].str.contains(\"0|no dirt|zero|none|nothing\", regex = True, case = False)*1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an article dictionary\n",
    "\n",
    "total_count = 301\n",
    "\n",
    "#storing article level information by article number\n",
    "article_dic = {}\n",
    "\n",
    "#article number maps to \n",
    "#fact checker eval, the day, and the type (1,2,3,4,5)\n",
    "        \n",
    "control_agg = control_data.groupby(['day'])[rel_likert].mean()\n",
    "\n",
    "these_days = set(control_agg.index)\n",
    "\n",
    "for this_it in answers.index:\n",
    "    \n",
    "    for col_num, col_mode in enumerate(vote_list):\n",
    "        \n",
    "        fact_eval = answers.loc[this_it,col_mode]\n",
    "        \n",
    "        fact_veracity = np.round(answers.loc[this_it,fact_likert[col_num]], decimals = 2)\n",
    "        \n",
    "        article_dic[total_count] = (fact_eval,fact_veracity,this_it,col_num + 1)\n",
    "        \n",
    "        total_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a useful function to convert answers\n",
    "def convert_answer(a_str):\n",
    "    \n",
    "    if \"are evaluating is factually accurate\" in a_str.lower():\n",
    "        return \"t\"\n",
    "    elif \"true, false, or misleading\" in a_str.lower():\n",
    "        return \"c\"\n",
    "    elif \"misleading and/or false\" in a_str.lower():\n",
    "        return \"f\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1: ('CND', 2.17, 'nov 13', 1),\n",
    "\n",
    "#getting EVERY fact checker response by article\n",
    "article_all = {}\n",
    "\n",
    "for key in article_dic.keys():\n",
    "    \n",
    "    new_list = []\n",
    "    \n",
    "    _,_,day,col = article_dic[key]\n",
    "    \n",
    "    sub_ind = answers_individual[answers_individual.day == day]\n",
    "    \n",
    "    cat_list = list(sub_ind[eval_cols[col - 1]])\n",
    "    cat_list = [convert_answer(x) for x in cat_list]\n",
    "    \n",
    "    ver_resp = list(sub_ind[rel_likert[col - 1]].str[0])\n",
    "    \n",
    "    while len(cat_list) < 6:\n",
    "        cat_list.append(None)\n",
    "        \n",
    "    while len(ver_resp) < 6:\n",
    "        ver_resp.append(None)\n",
    "    \n",
    "    new_list.extend(cat_list)\n",
    "    new_list.extend(ver_resp)\n",
    "\n",
    "    article_all[key] = new_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving this\n",
    "pickle.dump(article_all, open( data_pickles + 'article_all_noncovid.p', \"wb\" ))  \n",
    "pickle.dump(article_dic, open( data_pickles + 'article_dic_noncovid.p', \"wb\" ))  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating a df article dictionary\n",
    "### basically look up each article num and then pull all the responses\n",
    "\n",
    "response_dic = {}\n",
    "\n",
    "for key_it in article_dic.keys():\n",
    "    \n",
    "    this_day = article_dic[key_it][2]\n",
    "    \n",
    "    article_col = article_dic[key_it][3]\n",
    "    #print(this_day in control_data.day.unique())\n",
    "    if this_day in control_data.day.unique():\n",
    "       \n",
    "        control_data_day = control_data[control_data.day == this_day].copy()\n",
    "        \n",
    "        control_data_day['eval_col'] = control_data_day[eval_cols[article_col - 1]]\n",
    "        \n",
    "        control_data_day = control_data_day[control_data_day['eval_col'] != 'not asked']\n",
    "        \n",
    "        #translate responses\n",
    "        control_data_day['eval_col'] = control_data_day['eval_col'].apply(convert_answer)\n",
    "        \n",
    "        control_data_day['eval_likert'] = control_data_day[rel_likert[article_col - 1]]\n",
    "        \n",
    "        response_dic[key_it] = control_data_day\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(response_dic, open(data_pickles + \"response_dic_noncovid.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2769"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_size = 0\n",
    "\n",
    "for the_key in response_dic.keys():\n",
    "\n",
    "    all_size += response_dic[the_key].shape[0]\n",
    "    \n",
    "all_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function draws a sample answer given a dataframe\n",
    "#it returns a list that has\n",
    "\n",
    "#for a total of num\n",
    "#each categorical evaluation\n",
    "#likert evaluation\n",
    "#political score for that individual\n",
    "\n",
    "def draw_sample(df, num = 1):\n",
    "    \n",
    "    #dictionary to convert partisanship to numbers\n",
    "    partisan_dic = {'Conservative':2,\n",
    "                     'Extremely Conservative':3,\n",
    "                     'Extremely Liberal':-3,\n",
    "                     \"Haven't thought much about it\": 0, #treating these the same as moderate (for now)\n",
    "                     'Liberal':-2,\n",
    "                     'Moderate: Middle of the road':0,\n",
    "                     'Slightly Conservative':1,\n",
    "                     'Slightly Liberal':-1}\n",
    "    \n",
    "    samples = df.sample(n = num, replace=True).copy()\n",
    "    \n",
    "    output = []\n",
    "    \n",
    "    response_list_col = list(samples.loc[:,'eval_col'])\n",
    "    response_list_veracity = list(samples.loc[:,'eval_likert'])\n",
    "    response_list_ideology = list(samples.loc[:,'Q_Ideology'])\n",
    "    response_list_ideology = [partisan_dic[x] for x in response_list_ideology]\n",
    "    resp_pol_know = list(samples.loc[:,'pol_knowledge'])\n",
    "    \n",
    "    samples['tot_crt'] = np.nan\n",
    "    resp_crt = list(samples.loc[:,'tot_crt'])\n",
    "    \n",
    "    output.extend(response_list_col)\n",
    "    output.extend(response_list_veracity)\n",
    "    output.extend(response_list_ideology)\n",
    "    output.extend(resp_pol_know)\n",
    "    output.extend(resp_crt)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating train and test data\n",
    "\n",
    "## First step is divide articles into Training and test\n",
    "\n",
    "## false and true are determined by mode of fact checkers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_remover(set_input):\n",
    "    \n",
    "    return_set = []\n",
    "    \n",
    "    for article in set_input:\n",
    "        \n",
    "        if 0 < article % 5 < 4:\n",
    "            \n",
    "            return_set.append(article)\n",
    "            \n",
    "        \n",
    "    return set(return_set)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_list = []\n",
    "true_list = []\n",
    "CND_list = []\n",
    "\n",
    "for this_key in article_dic.keys():\n",
    "    \n",
    "    rating, _, _, _ = article_dic[this_key]\n",
    "    \n",
    "    \n",
    "    if rating == 'FM':\n",
    "        \n",
    "        false_list.append(this_key)\n",
    "        \n",
    "    elif rating == 'CND':\n",
    "        \n",
    "        CND_list.append(this_key)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        true_list.append(this_key)\n",
    "        \n",
    "false_list = article_remover(false_list)\n",
    "true_list = article_remover(true_list)\n",
    "CND_list = article_remover(CND_list)\n",
    "\n",
    "false_set = set(false_list)\n",
    "true_set = set(true_list)\n",
    "cnd_set  = set(CND_list)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NOTE - Correct Cols are created below!\n",
    "correct_cols = ['correct_1',\n",
    "                'correct_2',\n",
    "                'correct_3',\n",
    "                'correct_4',\n",
    "                'correct_5']\n",
    "\n",
    "ratio_list = []\n",
    "count_list = []\n",
    "sum_list = []\n",
    "\n",
    "for x in correct_cols:\n",
    "    \n",
    "    count = control_data.groupby('day')[x].count()\n",
    "    sum_tot = control_data.groupby('day')[x].sum()\n",
    "    ratio = sum_tot/count\n",
    "    \n",
    "    count_list.append(count)\n",
    "    sum_list.append(sum_tot)\n",
    "    ratio_list.append(ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an article dictionary for the percetage correct by article\n",
    "\n",
    "vote_list = ['Eval_1_mode',\n",
    "             'Eval_2_mode',\n",
    "             'Eval_3_mode',\n",
    "             'Eval_4_mode',\n",
    "             'Eval_5_mode']\n",
    "\n",
    "eval_cols = ['clean_Q13_Eval_1',\n",
    "             'clean_Q13_Eval_2',\n",
    "             'clean_Q13_Eval_3',\n",
    "             'clean_Q13_Eval_4',\n",
    "             'clean_Q13_Eval_5']\n",
    "\n",
    "total_count = 300\n",
    "\n",
    "#storing article level information by article number\n",
    "diff_dic = {}\n",
    "\n",
    "#article number maps to \n",
    "#fact checker eval, the day, and the type (1,2,3,4,5)\n",
    "\n",
    "\n",
    "for this_it in answers.index[:]:\n",
    "    \n",
    "    for col_num, col_mode in enumerate(vote_list):\n",
    "        \n",
    "        diff_dic[total_count] = (ratio_list[col_num][this_it], this_it, col_num+1,col_mode)\n",
    "        \n",
    "        total_count += 1\n",
    "        \n",
    "        #control_mode = control_data.groupby(['day'])[eval_cols].agg(lambda x: tuple(stats.mode(x)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_sorter(a_set, the_dic):\n",
    "    \n",
    "    a_list = []\n",
    "    \n",
    "    for article in a_set:\n",
    "        \n",
    "        a_list.append(list((article,the_dic[article][0])))\n",
    "        \n",
    "    a_list.sort(key = lambda x: x[1])\n",
    "    \n",
    "    evens = list(range(0,len(a_list),2))\n",
    "    odds = list(range(1,len(a_list),2))\n",
    "    \n",
    "    train_list = [a_list[x][0] for x in evens]\n",
    "    test_list = [a_list[x][0] for x in odds]\n",
    "    \n",
    "    assert len(set(test_list).intersection(set(train_list))) == 0\n",
    "    \n",
    "        \n",
    "    return test_list, train_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_sorter_adv(a_set,percent, the_dic):\n",
    "    #percent is for the number to go to the smaller of two sets\n",
    "    \n",
    "    num_small = round(len(a_set)*percent) \n",
    "    \n",
    "    if num_small == 0:\n",
    "        \n",
    "        test_list = []\n",
    "        train_list = list(a_set)\n",
    "    \n",
    "        return test_list, train_list\n",
    "    \n",
    "    \n",
    "    a_list = [(article,the_dic[article][0]) for article in a_set]\n",
    "    \n",
    "    a_list.sort(key = lambda x: x[1])\n",
    "    \n",
    "    evens = list(range(0,len(a_list),2))\n",
    "    odds = list(range(1,len(a_list),2))\n",
    "    \n",
    "    test_list = []\n",
    "    train_list = []\n",
    "\n",
    "    step = round(len(a_set)/num_small)\n",
    "\n",
    "    for num, obj in enumerate(a_set):\n",
    "\n",
    "            if num % step == 0:\n",
    "\n",
    "                test_list.append(obj)\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                \n",
    "                train_list.append(obj)  \n",
    "                \n",
    "    while len(test_list) > num_small:\n",
    "        \n",
    "        hold = test_list.pop()\n",
    "        train_list.append(hold)\n",
    "        \n",
    "    while len(train_list) > (len(a_set) - num_small):\n",
    "        \n",
    "        hold = train_list.pop()\n",
    "        test_list.append(hold)\n",
    "    \n",
    "    assert len(set(test_list).intersection(set(train_list))) == 0\n",
    "    \n",
    "    assert len(train_list) + len(test_list)  == len(a_set)\n",
    "    \n",
    "        \n",
    "    return test_list, train_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "test_false, train_false = article_sorter_adv(false_set,.2,diff_dic)\n",
    "test_true, train_true = article_sorter_adv(true_set,.2,diff_dic)\n",
    "\n",
    "val_false, train_false = article_sorter_adv(train_false,.2,diff_dic)\n",
    "val_true, train_true = article_sorter_adv(train_true,.2,diff_dic)\n",
    "\n",
    "\n",
    "test_set = test_false.copy()\n",
    "test_set.extend(test_true)\n",
    "\n",
    "train_set = train_false.copy()\n",
    "train_set.extend(train_true)\n",
    "\n",
    "val_set = val_false.copy()\n",
    "val_set.extend(val_true)\n",
    "\n",
    "#residual articles\n",
    "all_articles_set = set(article_all.keys())\n",
    "all_articles_set = article_remover(all_articles_set)\n",
    "all_articles_set = all_articles_set - set(false_set) - set(true_set)\n",
    "\n",
    "no_assign_test, no_assign_train = article_sorter_adv(all_articles_set,.2,diff_dic)\n",
    "no_assign_val, no_assign_train = article_sorter_adv(no_assign_train,.2,diff_dic)\n",
    "\n",
    "test_set.extend(no_assign_test)\n",
    "train_set.extend(no_assign_train)\n",
    "val_set.extend(no_assign_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(test_set, open(data_pickles + \"test_set_noncovid.p\", \"wb\"))\n",
    "pickle.dump(val_set, open(data_pickles + \"val_set_noncovid.p\", \"wb\"))\n",
    "pickle.dump(train_set, open(data_pickles + \"train_set_noncovid.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_set) + len(train_set) + len(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(false_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5 + 5+ 1+ 1+ + 2 +2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating training and test set\n",
    "## Here, generating column names for the fake data I am constructing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, sorting articles\n",
    "\n",
    "num_resp= 60\n",
    "\n",
    "num_fc = 6\n",
    "\n",
    "col_list_df = []\n",
    "\n",
    "types = ['resp_cat_', \"resp_veracity_\",\"ideology_resp_\",\"pol_know_\",\"crt_\", \"fc_cat_\", \"fc_veracity_\"]\n",
    "\n",
    "for it in range(num_resp):\n",
    "    \n",
    "    col_name = types[0] + str(it)\n",
    "    col_list_df.append(col_name)\n",
    "    \n",
    "for it in range(num_resp):\n",
    "    \n",
    "    col_name = types[1] + str(it)\n",
    "    col_list_df.append(col_name)\n",
    "\n",
    "for it in range(num_resp):\n",
    "    \n",
    "    col_name = types[2] + str(it)\n",
    "    col_list_df.append(col_name)\n",
    "    \n",
    "    \n",
    "for it in range(num_resp):\n",
    "    \n",
    "    col_name = types[3] + str(it)\n",
    "    col_list_df.append(col_name)\n",
    "    \n",
    "    \n",
    "for it in range(num_resp): \n",
    "    \n",
    "    col_name = types[4] + str(it)\n",
    "    col_list_df.append(col_name)\n",
    "    \n",
    "    \n",
    "for it in range(num_fc):\n",
    "    \n",
    "    col_name = types[5] + str(it)\n",
    "    col_list_df.append(col_name)\n",
    "    \n",
    "for it in range(num_fc):\n",
    "    \n",
    "    col_name = types[6] + str(it)\n",
    "    col_list_df.append(col_name)\n",
    "    \n",
    "    \n",
    "    \n",
    "col_list_df.append(\"article_num\")\n",
    "\n",
    "col_list_sets = [\"mode\",\"in_robust_mode\",\"no_false\", \\\n",
    "           \"all_true_set\", \"any_true_set\",\"no_true\",\"all_false_set\",\"any_false_set\"]\n",
    "\n",
    "col_list_df.extend(col_list_sets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use training and test sets\n",
    "\n",
    "1. All articles that are in one of the six sets (three for false and three for true) is cleanly split between training and test, with unassigned articles also randomly split.\n",
    "\n",
    "2. For a specific balance in training/test, they can then be resampled at the appropriate weights given whatever criteria (mode, robust mode, etc) is being used, to create a more balanced training and test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Next, generating training data. \n",
    "np.random.seed(42)\n",
    "\n",
    "num_training_obs = len(train_set)*500\n",
    "\n",
    "num_responses = num_resp\n",
    "\n",
    "all_sims = []\n",
    "\n",
    "for this_iter in range(num_training_obs):\n",
    "    \n",
    "    this_sim = []\n",
    "    \n",
    "    the_article_num = np.random.choice(train_set, size = 1, replace = False)[0]\n",
    "        \n",
    "    #the responses\n",
    "    this_df = response_dic[the_article_num]\n",
    "    \n",
    "    #fc response\n",
    "    mode_est, _ , date, art_type = article_dic[the_article_num]\n",
    "      \n",
    "    #drawing a sample\n",
    "    resp_list = draw_sample(this_df, num = num_responses)\n",
    "    \n",
    "    #getting fact checker information\n",
    "    fc_list = article_all[the_article_num]\n",
    "    \n",
    "    this_sim.extend(resp_list)\n",
    "    this_sim.extend(fc_list)\n",
    "    this_sim.append(int(the_article_num))\n",
    "    \n",
    "    robust_mode = None\n",
    "    \n",
    "    no_contradiction_true = np.nan\n",
    "    all_true = np.nan\n",
    "    any_true = np.nan\n",
    "    \n",
    "    no_contradiction_false = np.nan\n",
    "    all_false = np.nan\n",
    "    any_false = np.nan\n",
    "    \n",
    "    output_list = [mode_est, robust_mode]\n",
    "    output_list_true = [no_contradiction_true,all_true,any_true]\n",
    "    output_list_false = [no_contradiction_false, all_false, any_false]\n",
    "    \n",
    "    output_list.extend(output_list_true)\n",
    "    output_list.extend(output_list_false)\n",
    "    \n",
    "    this_sim.extend(output_list)\n",
    "    \n",
    "    all_sims.append(this_sim)\n",
    "\n",
    "#training data\n",
    "train_data = pd.DataFrame(all_sims, columns = col_list_df) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Next, generating val data. \n",
    "np.random.seed(42)\n",
    "\n",
    "num_training_obs = len(val_set)*500\n",
    "\n",
    "num_responses = num_resp\n",
    "\n",
    "all_sims = []\n",
    "\n",
    "for this_iter in range(num_training_obs):\n",
    "    \n",
    "    this_sim = []\n",
    "    \n",
    "    the_article_num = np.random.choice(val_set, size = 1, replace = False)[0]\n",
    "        \n",
    "    #the responses\n",
    "    this_df = response_dic[the_article_num]\n",
    "    \n",
    "    #fc response\n",
    "    mode_est, _ , date, art_type = article_dic[the_article_num]\n",
    "      \n",
    "    #drawing a sample\n",
    "    resp_list = draw_sample(this_df, num = num_responses)\n",
    "    \n",
    "    #getting fact checker information\n",
    "    fc_list = article_all[the_article_num]\n",
    "    \n",
    "    this_sim.extend(resp_list)\n",
    "    this_sim.extend(fc_list)\n",
    "    this_sim.append(int(the_article_num))\n",
    "    \n",
    "    robust_mode = None\n",
    "\n",
    "    no_contradiction_true = np.nan\n",
    "    all_true = np.nan\n",
    "    any_true = np.nan\n",
    "    \n",
    "    no_contradiction_false = np.nan\n",
    "    all_false = np.nan\n",
    "    any_false = np.nan\n",
    "    \n",
    "    output_list = [mode_est, robust_mode]\n",
    "    output_list_true = [no_contradiction_true,all_true,any_true]\n",
    "    output_list_false = [no_contradiction_false, all_false, any_false]\n",
    "    \n",
    "    output_list.extend(output_list_true)\n",
    "    output_list.extend(output_list_false)\n",
    "    \n",
    "    this_sim.extend(output_list)\n",
    "    \n",
    "    all_sims.append(this_sim)\n",
    "\n",
    "#training data\n",
    "val_data = pd.DataFrame(all_sims, columns = col_list_df) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Next, generating test data. \n",
    "np.random.seed(42)\n",
    "\n",
    "num_test_obs = len(test_set)*500\n",
    "\n",
    "num_responses = num_resp\n",
    "\n",
    "all_sims = []\n",
    "\n",
    "for this_iter in range(num_test_obs):\n",
    "    \n",
    "    this_sim = []\n",
    "    \n",
    "    the_article_num = np.random.choice(test_set, size = 1, replace = False)[0]\n",
    "        \n",
    "    #the responses\n",
    "    this_df = response_dic[the_article_num]\n",
    "    \n",
    "    #fc response\n",
    "    mode_est, _ , date, art_type = article_dic[the_article_num]\n",
    "      \n",
    "    #drawing a sample\n",
    "    resp_list = draw_sample(this_df, num = num_responses)\n",
    "    \n",
    "    #getting fact checker information\n",
    "    fc_list = article_all[the_article_num]\n",
    "    \n",
    "    this_sim.extend(resp_list)\n",
    "    this_sim.extend(fc_list)\n",
    "    this_sim.append(int(the_article_num))\n",
    "    \n",
    "    robust_mode = None\n",
    "    \n",
    "    no_contradiction_true = np.nan\n",
    "    all_true = np.nan\n",
    "    any_true = np.nan\n",
    "    \n",
    "    no_contradiction_false = np.nan\n",
    "    all_false = np.nan\n",
    "    any_false = np.nan\n",
    "    \n",
    "    output_list = [mode_est, robust_mode]\n",
    "    output_list_true = [no_contradiction_true,all_true,any_true]\n",
    "    output_list_false = [no_contradiction_false, all_false, any_false]\n",
    "    \n",
    "    output_list.extend(output_list_true)\n",
    "    output_list.extend(output_list_false)\n",
    "    \n",
    "    this_sim.extend(output_list)\n",
    "    \n",
    "    all_sims.append(this_sim)\n",
    "\n",
    "#training data\n",
    "test_data = pd.DataFrame(all_sims, columns = col_list_df) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving data\n",
    "train_data.to_csv(prepared_data + \"train_data_large_noncovid.csv\", index = False)\n",
    "val_data.to_csv(prepared_data + \"val_data_large_noncovid.csv\", index = False)\n",
    "test_data.to_csv(prepared_data + \"test_data_large_noncovid.csv\", index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
