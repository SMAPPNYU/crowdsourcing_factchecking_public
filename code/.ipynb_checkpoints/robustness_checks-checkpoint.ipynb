{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author of code: William Godel\n",
    "\n",
    "Date: 07/02\n",
    "\n",
    "Purpose: to test performance of models on alternative data distributions\n",
    "\n",
    "## Data IN: \n",
    "\n",
    "test_data_98.csv\n",
    "\n",
    "test_data_90.csv\n",
    "\n",
    "test_data_75.csv\n",
    "\n",
    "test_data_10.csv\n",
    "\n",
    "\n",
    "test_data_98_highpol.csv\n",
    "\n",
    "test_data_90_highpol.csv\n",
    "\n",
    "test_data_75_highpol.csv\n",
    "\n",
    "test_data_10_highpol.csv\n",
    "\n",
    "grid_rf.p\n",
    "\n",
    "model_nn_highpol.pt\n",
    "\n",
    "model_large.pt\n",
    "\n",
    "## Data OUT:\n",
    "\n",
    "no data out, just statistics\n",
    "\n",
    "\n",
    "Machine: My laptop or Imac\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "laptop = True\n",
    "import pickle\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "from functions import count_mode, bayes_probs, bayes_binary\n",
    "from ml_functions import feature_creation\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#crowd_size, feature_transform\n",
    "\n",
    "from path import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_98 = pd.read_csv(prepared_data + \"test_data_98.csv\")\n",
    "test_data_90 = pd.read_csv(prepared_data + \"test_data_90.csv\")\n",
    "test_data_75 = pd.read_csv(prepared_data + \"test_data_75.csv\")\n",
    "test_data_10 = pd.read_csv(prepared_data + \"test_data_10.csv\")\n",
    "\n",
    "test_data_98_target = test_data_98['mode'] == 'FM'\n",
    "test_data_90_target = test_data_90['mode'] == 'FM'\n",
    "test_data_75_target = test_data_75['mode'] == 'FM'\n",
    "test_data_10_target = test_data_10['mode'] == 'FM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_data_98 = feature_creation(test_data_98)\n",
    "X_test_data_90 = feature_creation(test_data_90)\n",
    "X_test_data_75 = feature_creation(test_data_75)\n",
    "X_test_data_10 = feature_creation(test_data_10)\n",
    "\n",
    "X_test_data_98_large = feature_creation(test_data_98, crowd_num = 25)\n",
    "X_test_data_90_large = feature_creation(test_data_90, crowd_num = 25)\n",
    "X_test_data_75_large = feature_creation(test_data_75, crowd_num = 25)\n",
    "X_test_data_10_large = feature_creation(test_data_10, crowd_num = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = []\n",
    "\n",
    "resp_var = [x for x in X_test_data_98.columns if 'resp_veracity_' in x]\n",
    "new_cols = [x for x in X_test_data_98.columns if 'new' in x]\n",
    "\n",
    "#numeric_features.extend(resp_var)\n",
    "numeric_features.extend(resp_var) #rempve this if it doesn't work\n",
    "numeric_features.extend(new_cols)\n",
    "numeric_features.extend([\"crowd_means\",'crowd_median','crowd_full_range', 'crowd_IQR_range', \\\n",
    "                         'crowd_variance', 'crowd_bayes'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_eval(preds, truth):\n",
    "    \n",
    "    if True in np.unique(truth):\n",
    "        \n",
    "        condition_pos = True\n",
    "        condition_neg = False\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        condition_pos = 1\n",
    "        condition_neg = 0\n",
    "        \n",
    "    con_pos_count = np.sum(truth == condition_pos)\n",
    "    con_neg_count = np.sum(truth == condition_neg)\n",
    "        \n",
    "    accuracy = np.sum((preds == truth))/truth.size\n",
    "    \n",
    "    True_pos = np.sum(preds[truth == condition_pos] == truth[truth ==condition_pos])\n",
    "    False_pos = np.sum(preds[truth == condition_pos] != truth[truth ==condition_pos])\n",
    "    \n",
    "    True_neg = np.sum(preds[truth == condition_neg] == truth[truth == condition_neg])\n",
    "    False_neg = np.sum(preds[truth == condition_neg] != truth[truth == condition_neg])\n",
    "    \n",
    "    \n",
    "    return accuracy, True_pos, False_pos, True_neg, False_neg\n",
    "    \n",
    "    \n",
    "def conf_perc(a_list):\n",
    "    \n",
    "    total_pos = sum(a_list[2:4])\n",
    "    total_neg = sum(a_list[4:])\n",
    "    print(\"TP\", \"FP\", \"TN\", \"FN\")\n",
    "    return a_list[2]/total_pos, a_list[3]/total_pos, a_list[4]/total_neg, a_list[5]/total_neg\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "### standard crowd\n",
    "### size 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_dic_98 = {}\n",
    "performance_dic_90 = {}\n",
    "performance_dic_75 = {}\n",
    "performance_dic_10 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_rf = pickle.load(open(models + 'grid_rf.p', \"rb\" ))\n",
    "\n",
    "algo = grid_rf\n",
    "\n",
    "algo_name = \"randomforest_10\"\n",
    "\n",
    "all_results = [algo_name]\n",
    "results = list(conf_eval(algo.predict(X_test_data_98), test_data_98_target))\n",
    "all_results.extend(results)\n",
    "performance_dic_98[algo_name] = all_results\n",
    "\n",
    "all_results = [algo_name]\n",
    "results = list(conf_eval(algo.predict(X_test_data_90), test_data_90_target))\n",
    "all_results.extend(results)\n",
    "performance_dic_90[algo_name] = all_results\n",
    "\n",
    "all_results = [algo_name]\n",
    "results = list(conf_eval(algo.predict(X_test_data_75), test_data_75_target))\n",
    "all_results.extend(results)\n",
    "performance_dic_75[algo_name] = all_results\n",
    "\n",
    "results = [algo_name]\n",
    "all_results = list(conf_eval(algo.predict(X_test_data_10), test_data_10_target))\n",
    "all_results.extend(results)\n",
    "performance_dic_10[algo_name] = all_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP FP TN FN\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.38421052631578945,\n",
       " 0.6157894736842106,\n",
       " 0.7682062298603652,\n",
       " 0.2317937701396348)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_perc(performance_dic_98['randomforest_10'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['randomforest_10', 0.7605263157894737, 73, 117, 7152, 2158]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance_dic_98['randomforest_10']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualified Crowds\n",
    "\n",
    "### loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_98_highpk = pd.read_csv(prepared_data + 'test_data_98_highpol.csv')\n",
    "test_data_90_highpk = pd.read_csv(prepared_data + \"test_data_90_highpol.csv\")\n",
    "test_data_75_highpk = pd.read_csv(prepared_data + \"test_data_75_highpol.csv\")\n",
    "test_data_10_highpk = pd.read_csv(prepared_data + \"test_data_10_highpol.csv\")\n",
    "\n",
    "\n",
    "X_test_data_98_highpk = feature_creation(test_data_98_highpk)\n",
    "X_test_data_90_highpk = feature_creation(test_data_90_highpk)\n",
    "X_test_data_75_highpk = feature_creation(test_data_75_highpk)\n",
    "X_test_data_10_highpk = feature_creation(test_data_10_highpk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dim):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(emb_dim, 500) \n",
    "        self.fc2 = nn.Linear(500, 100)\n",
    "        self.fc3 = nn.Linear(100, 25)\n",
    "        self.fc4 = nn.Linear(25, 25)\n",
    "        self.fc5 = nn.Linear(25, 10)\n",
    "        self.fc6 = nn.Linear(10, 1)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class data_loader(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list.values\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        data = self.data_list[key]\n",
    "        label = self.target_list[key]\n",
    "        return [data, label]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "emb_dim = 61   \n",
    "net_large = Net(emb_dim)\n",
    "net_large.load_state_dict(torch.load(models + \"model_nn_highpol.pt\"))\n",
    "net_large.eval()\n",
    "\n",
    "# Preparing the data\n",
    "y_data_bin_large = np.where(test_data_98_target == True, 1, 0)\n",
    "data = X_test_data_98_highpk\n",
    "nn_list_large = [\"nn_10_pk\"]\n",
    "test = StandardScaler()\n",
    "test.fit(data[numeric_features])\n",
    "#training data\n",
    "X_train_stand_large = data.copy()\n",
    "X_train_stand_large[numeric_features] = test.transform(data[numeric_features])\n",
    "train_dat_nn_large = data_loader(X_train_stand_large,y_data_bin_large)\n",
    "nn_preds  = torch.sigmoid(net_large(torch.tensor(train_dat_nn_large[:][0]).float()))\n",
    "nn_preds = np.where(nn_preds > .5, 1,0).reshape(-1,)\n",
    "all_results = list(conf_eval(nn_preds, y_data_bin_large))\n",
    "\n",
    "nn_list_large.extend(all_results)\n",
    "performance_dic_98['nn_10_pk'] = nn_list_large\n",
    "\n",
    "\n",
    "\n",
    "# Preparing the data\n",
    "y_data_bin_large = np.where(test_data_90_target == True, 1, 0)\n",
    "data = X_test_data_90_highpk\n",
    "nn_list_large = [\"nn_25\"]\n",
    "test = StandardScaler()\n",
    "test.fit(data[numeric_features])\n",
    "#training data\n",
    "X_train_stand_large = data.copy()\n",
    "X_train_stand_large[numeric_features] = test.transform(data[numeric_features])\n",
    "train_dat_nn_large = data_loader(X_train_stand_large,y_data_bin_large)\n",
    "nn_preds  = torch.sigmoid(net_large(torch.tensor(train_dat_nn_large[:][0]).float()))\n",
    "nn_preds = np.where(nn_preds > .5, 1,0).reshape(-1,)\n",
    "all_results = list(conf_eval(nn_preds, y_data_bin_large))\n",
    "\n",
    "nn_list_large.extend(all_results)\n",
    "performance_dic_90['nn_10_pk'] = nn_list_large\n",
    "\n",
    "\n",
    "# Preparing the data\n",
    "y_data_bin_large = np.where(test_data_75_target == True, 1, 0)\n",
    "data = X_test_data_75_highpk\n",
    "nn_list_large = [\"nn_10_pk\"]\n",
    "test = StandardScaler()\n",
    "test.fit(data[numeric_features])\n",
    "#training data\n",
    "X_train_stand_large = data.copy()\n",
    "X_train_stand_large[numeric_features] = test.transform(data[numeric_features])\n",
    "train_dat_nn_large = data_loader(X_train_stand_large,y_data_bin_large)\n",
    "nn_preds  = torch.sigmoid(net_large(torch.tensor(train_dat_nn_large[:][0]).float()))\n",
    "nn_preds = np.where(nn_preds > .5, 1,0).reshape(-1,)\n",
    "all_results = list(conf_eval(nn_preds, y_data_bin_large))\n",
    "\n",
    "nn_list_large.extend(all_results)\n",
    "performance_dic_75['nn_10_pk'] = nn_list_large\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Preparing the data\n",
    "y_data_bin_large = np.where(test_data_10_target == True, 1, 0)\n",
    "data = X_test_data_10_highpk\n",
    "nn_list_large = [\"nn_10_pk\"]\n",
    "test = StandardScaler()\n",
    "test.fit(data[numeric_features])\n",
    "#training data\n",
    "X_train_stand_large = data.copy()\n",
    "X_train_stand_large[numeric_features] = test.transform(data[numeric_features])\n",
    "train_dat_nn_large = data_loader(X_train_stand_large,y_data_bin_large)\n",
    "nn_preds  = torch.sigmoid(net_large(torch.tensor(train_dat_nn_large[:][0]).float()))\n",
    "nn_preds = np.where(nn_preds > .5, 1,0).reshape(-1,)\n",
    "all_results = list(conf_eval(nn_preds, y_data_bin_large))\n",
    "\n",
    "nn_list_large.extend(all_results)\n",
    "performance_dic_10['nn_10_pk'] = nn_list_large\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nn_10_pk', 0.4709473684210526, 3656, 4894, 818, 132]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance_dic_10['nn_10_pk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'randomforest_10': ['randomforest_10',\n",
       "  0.7349194652068639,\n",
       "  359,\n",
       "  590,\n",
       "  6622,\n",
       "  1928],\n",
       " 'nn_10_pk': ['nn_25', 0.7534477313401411, 486, 463, 6671, 1879]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance_dic_90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network\n",
    "### Large crowd\n",
    "### size 25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features_large = []\n",
    "\n",
    "resp_var = [x for x in X_test_data_98_large.columns if 'resp_veracity_' in x]\n",
    "new_cols = [x for x in X_test_data_98_large.columns if 'new' in x]\n",
    "\n",
    "#numeric_features.extend(resp_var)\n",
    "numeric_features_large.extend(resp_var) #rempve this if it doesn't work\n",
    "numeric_features_large.extend(new_cols)\n",
    "numeric_features_large.extend([\"crowd_means\",'crowd_median','crowd_full_range', 'crowd_IQR_range', \\\n",
    "                         'crowd_variance', 'crowd_bayes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "emb_dim = 121   \n",
    "net_large = Net(emb_dim)\n",
    "net_large.load_state_dict(torch.load(models + \"model_large.pt\"))\n",
    "net_large.eval()\n",
    "\n",
    "\n",
    "\n",
    "# Preparing the data\n",
    "y_data_bin_large = np.where(test_data_98_target == True, 1, 0)\n",
    "data = X_test_data_98_large\n",
    "nn_list_large = [\"nn_25\"]\n",
    "test = StandardScaler()\n",
    "test.fit(data[numeric_features_large])\n",
    "#training data\n",
    "X_train_stand_large = data.copy()\n",
    "X_train_stand_large[numeric_features_large] = test.transform(data[numeric_features_large])\n",
    "train_dat_nn_large = data_loader(X_train_stand_large,y_data_bin_large)\n",
    "nn_preds  = torch.sigmoid(net_large(torch.tensor(train_dat_nn_large[:][0]).float()))\n",
    "nn_preds = np.where(nn_preds > .5, 1,0).reshape(-1,)\n",
    "all_results = list(conf_eval(nn_preds, y_data_bin_large))\n",
    "\n",
    "nn_list_large.extend(all_results)\n",
    "performance_dic_98['nn_25'] = nn_list_large\n",
    "\n",
    "\n",
    "\n",
    "# Preparing the data\n",
    "y_data_bin_large = np.where(test_data_90_target == True, 1, 0)\n",
    "data = X_test_data_90_large\n",
    "nn_list_large = [\"nn_25\"]\n",
    "test = StandardScaler()\n",
    "test.fit(data[numeric_features_large])\n",
    "#training data\n",
    "X_train_stand_large = data.copy()\n",
    "X_train_stand_large[numeric_features_large] = test.transform(data[numeric_features_large])\n",
    "train_dat_nn_large = data_loader(X_train_stand_large,y_data_bin_large)\n",
    "nn_preds  = torch.sigmoid(net_large(torch.tensor(train_dat_nn_large[:][0]).float()))\n",
    "nn_preds = np.where(nn_preds > .5, 1,0).reshape(-1,)\n",
    "all_results = list(conf_eval(nn_preds, y_data_bin_large))\n",
    "\n",
    "nn_list_large.extend(all_results)\n",
    "performance_dic_90['nn_25'] = nn_list_large\n",
    "\n",
    "\n",
    "\n",
    "# Preparing the data\n",
    "y_data_bin_large = np.where(test_data_75_target == True, 1, 0)\n",
    "data = X_test_data_75_large\n",
    "nn_list_large = [\"nn_25\"]\n",
    "test = StandardScaler()\n",
    "test.fit(data[numeric_features_large])\n",
    "#training data\n",
    "X_train_stand_large = data.copy()\n",
    "X_train_stand_large[numeric_features_large] = test.transform(data[numeric_features_large])\n",
    "train_dat_nn_large = data_loader(X_train_stand_large,y_data_bin_large)\n",
    "nn_preds  = torch.sigmoid(net_large(torch.tensor(train_dat_nn_large[:][0]).float()))\n",
    "nn_preds = np.where(nn_preds > .5, 1,0).reshape(-1,)\n",
    "all_results = list(conf_eval(nn_preds, y_data_bin_large))\n",
    "\n",
    "nn_list_large.extend(all_results)\n",
    "performance_dic_75['nn_25'] = nn_list_large\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Preparing the data\n",
    "y_data_bin_large = np.where(test_data_10_target == True, 1, 0)\n",
    "data = X_test_data_10_large\n",
    "nn_list_large = [\"nn_25\"]\n",
    "test = StandardScaler()\n",
    "test.fit(data[numeric_features_large])\n",
    "#training data\n",
    "X_train_stand_large = data.copy()\n",
    "X_train_stand_large[numeric_features_large] = test.transform(data[numeric_features_large])\n",
    "train_dat_nn_large = data_loader(X_train_stand_large,y_data_bin_large)\n",
    "nn_preds  = torch.sigmoid(net_large(torch.tensor(train_dat_nn_large[:][0]).float()))\n",
    "nn_preds = np.where(nn_preds > .5, 1,0).reshape(-1,)\n",
    "all_results = list(conf_eval(nn_preds, y_data_bin_large))\n",
    "\n",
    "nn_list_large.extend(all_results)\n",
    "performance_dic_10['nn_25'] = nn_list_large\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nn_25', 0.4708421052631579, 3644, 4906, 829, 121]\n",
      "TP FP TN FN\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.42619883040935674,\n",
       " 0.5738011695906433,\n",
       " 0.8726315789473684,\n",
       " 0.12736842105263158)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(performance_dic_10['nn_25'])\n",
    "conf_perc(performance_dic_10['nn_25'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nn_25', 0.7433684210526316, 1249, 1126, 5813, 1312]\n",
      "TP FP TN FN\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5258947368421053,\n",
       " 0.47410526315789475,\n",
       " 0.8158596491228071,\n",
       " 0.184140350877193)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(performance_dic_75['nn_25'])\n",
    "conf_perc(performance_dic_75['nn_25'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nn_25', 0.77260764290978, 527, 422, 6812, 1738]\n",
      "TP FP TN FN\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5553213909378293,\n",
       " 0.4446786090621707,\n",
       " 0.7967251461988304,\n",
       " 0.20327485380116958)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(performance_dic_90['nn_25'])\n",
    "conf_perc(performance_dic_90['nn_25'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nn_25', 0.7830526315789473, 101, 89, 7338, 1972]\n",
      "TP FP TN FN\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.531578947368421,\n",
       " 0.46842105263157896,\n",
       " 0.7881847475832439,\n",
       " 0.21181525241675617)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(performance_dic_98['nn_25'])\n",
    "conf_perc(performance_dic_98['nn_25'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nn_10_pk', 0.4709473684210526, 3656, 4894, 818, 132]\n",
      "TP FP TN FN\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.42760233918128654,\n",
       " 0.5723976608187135,\n",
       " 0.8610526315789474,\n",
       " 0.13894736842105262)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(performance_dic_10['nn_10_pk'])\n",
    "conf_perc(performance_dic_10['nn_10_pk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nn_10_pk', 0.7293684210526316, 1234, 1141, 5695, 1430]\n",
      "TP FP TN FN\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.519578947368421,\n",
       " 0.48042105263157897,\n",
       " 0.7992982456140351,\n",
       " 0.2007017543859649)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(performance_dic_75['nn_10_pk'])\n",
    "conf_perc(performance_dic_75['nn_10_pk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nn_25', 0.7534477313401411, 486, 463, 6671, 1879]\n",
      "TP FP TN FN\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.512118018967334,\n",
       " 0.48788198103266595,\n",
       " 0.780233918128655,\n",
       " 0.21976608187134503)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(performance_dic_90['nn_10_pk'])\n",
    "conf_perc(performance_dic_90['nn_10_pk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nn_10_pk', 0.765578947368421, 101, 89, 7172, 2138]\n",
      "TP FP TN FN\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.531578947368421,\n",
       " 0.46842105263157896,\n",
       " 0.7703544575725026,\n",
       " 0.22964554242749732)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(performance_dic_98['nn_10_pk'])\n",
    "conf_perc(performance_dic_98['nn_10_pk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
