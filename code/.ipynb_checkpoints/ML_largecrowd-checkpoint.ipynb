{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author of code: William Godel \n",
    "\n",
    "Date: 07/02\n",
    "\n",
    "Purpose: to train ML models for larger crowds data, and build select crowds data\n",
    "\n",
    "## Data IN: \n",
    "\n",
    "train_data_large.csv\n",
    "\n",
    "val_data_large.csv\n",
    "\n",
    "train_data_large_covid.csv\n",
    "\n",
    "val_data_large_covid.csv\n",
    "\n",
    "train_data_large_noncovid.csv\n",
    "\n",
    "val_data_large_noncovid.csv\n",
    "\n",
    "\n",
    "## Data OUT:\n",
    "\n",
    "### models\n",
    "\n",
    "grid_rf_large.p\n",
    "\n",
    "grid_en_large.p\n",
    "\n",
    "model_large.pt\n",
    "\n",
    "Machine: My laptop or Imac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "laptop = True\n",
    "import pickle\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(filename='ML_large.log',level=logging.DEBUG)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import ElasticNet, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "from ml_functions import feature_creation, conf_eval, data_prep, test_model\n",
    "\n",
    "from functions import count_mode, bayes_probs, bayes_binary\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#crowd_size, feature_transform\n",
    "\n",
    "from path import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(prepared_data + \"train_data_large.csv\")\n",
    "val_data = pd.read_csv(prepared_data + \"val_data_large.csv\")\n",
    "\n",
    "\n",
    "train_data_covid = pd.read_csv(prepared_data + \"train_data_large_covid.csv\")\n",
    "val_data_covid = pd.read_csv(prepared_data + \"val_data_large_covid.csv\")\n",
    "\n",
    "\n",
    "train_data_noncovid = pd.read_csv(prepared_data + \"train_data_large_noncovid.csv\")\n",
    "val_data_noncovid = pd.read_csv(prepared_data + \"val_data_large_noncovid.csv\")\n",
    "\n",
    "\n",
    "\n",
    "train_data_large = train_data.append(train_data_covid)\n",
    "train_data_large = train_data_large.append(train_data_noncovid)\n",
    "\n",
    "train_data_large.reset_index(inplace = True, drop = True)\n",
    "\n",
    "\n",
    "val_data_large = val_data.append(val_data_covid)\n",
    "val_data_large = val_data_large.append(val_data_noncovid)\n",
    "\n",
    "val_data_large.reset_index(inplace = True, drop = True)\n",
    "\n",
    "\n",
    "train_target_large = train_data_large['mode'] == 'FM'\n",
    "val_target_large = val_data_large['mode'] == 'FM'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building ML pipelines:\n",
    "\n",
    "1. Elastic net \n",
    "2. Random Forest\n",
    "3. Gradient Boosted Trees\n",
    "4. NN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the data\n",
    "X_data = feature_creation(train_data_large, crowd_num = 25)\n",
    "Y_data = train_target_large\n",
    "\n",
    "X_data_val = feature_creation(val_data_large, crowd_num = 25)\n",
    "Y_data_val = val_target_large\n",
    "\n",
    "X_data_all = X_data.append(X_data_val)\n",
    "Y_data_all = Y_data.append(Y_data_val)\n",
    "\n",
    "X_data_all.reset_index(inplace = True, drop = True)\n",
    "Y_data_all.reset_index(inplace = True, drop = True)\n",
    "\n",
    "fold_list = [-1 for x in range(X_data.shape[0])]\n",
    "fold_list.extend([0 for x in range(X_data_val.shape[0])])\n",
    "\n",
    "test_fold = np.array(fold_list)\n",
    "ps = PredefinedSplit(test_fold)\n",
    "\n",
    "#encoding the label\n",
    "#le = LabelEncoder()\n",
    "#le.fit(Y_data)\n",
    "#Y_data = le.transform(Y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_large = pd.read_csv(prepared_data + \"test_data_large.csv\")\n",
    "\n",
    "test_target_large = test_data_large['mode'] == 'FM'\n",
    "\n",
    "X_data_test = feature_creation(test_data_large, crowd_num = 25)\n",
    "Y_data_test = test_target_large\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeric Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = []\n",
    "\n",
    "resp_var = [x for x in X_data.columns if 'resp_veracity_' in x]\n",
    "new_cols = [x for x in X_data.columns if 'new' in x]\n",
    "\n",
    "#numeric_features.extend(resp_var)\n",
    "numeric_features.extend(resp_var) #rempve this if it doesn't work\n",
    "numeric_features.extend(new_cols)\n",
    "numeric_features.extend([\"crowd_means\",'crowd_median','crowd_full_range', 'crowd_IQR_range', \\\n",
    "                         'crowd_variance', 'crowd_bayes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[('num', numeric_transformer, numeric_features)])\n",
    "\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', RandomForestClassifier())])\n",
    "\n",
    "parameteres = {'classifier__max_depth':[1,2,3,4,5,10,20,None], \\\n",
    "               'classifier__min_samples_split':[2,3,5,10], \\\n",
    "               'classifier__min_samples_leaf':[1,2,3,5,10], \\\n",
    "               'classifier__n_estimators':[100,250,300,400,500]}\n",
    "\n",
    "grid_rf = GridSearchCV(clf, scoring = 'accuracy', param_grid=parameteres, cv=ps)\n",
    "\n",
    "grid_rf.fit(X_data_all,Y_data_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(grid_rf, open(models + \"grid_rf_large.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elastic Net\n",
    "numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[('num', numeric_transformer, numeric_features)])\n",
    "\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', LogisticRegression(penalty = 'elasticnet', solver = 'saga'))])\n",
    "\n",
    "parameteres = {'classifier__l1_ratio':[0.9,.8,.05,.2,.15,0.1,1], 'classifier__C':[.1,.5,1,5,10,20]}\n",
    "\n",
    "grid_en = GridSearchCV(clf, scoring = 'accuracy', param_grid=parameteres, cv=ps)\n",
    "\n",
    "grid_en.fit(X_data_all,Y_data_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(grid_en, open(models + \"grid_en_large.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the data\n",
    "\n",
    "\n",
    "# Preparing the data\n",
    "#X_data = feature_creation(train_data_fakesource.iloc[:,:])\n",
    "#Y_data = train_target_large_lowcred[:]\n",
    "\n",
    "\n",
    "test = StandardScaler()\n",
    "test.fit(X_data[numeric_features])\n",
    "\n",
    "#training data\n",
    "X_train_stand = X_data.copy()\n",
    "X_train_stand[numeric_features] = test.transform(X_train_stand[numeric_features])\n",
    "\n",
    "#val data\n",
    "X_val_stand = X_data_val.copy()\n",
    "X_val_stand[numeric_features] = test.transform(X_val_stand[numeric_features])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(prepared_data + \"test_data_large.csv\")\n",
    "test_data_covid = pd.read_csv(prepared_data + \"test_data_large_covid.csv\")\n",
    "test_data_noncovid = pd.read_csv(prepared_data + \"test_data_large_noncovid.csv\")\n",
    "\n",
    "test_data_large = test_data.append(test_data_covid)\n",
    "test_data_large = test_data_large.append(test_data_noncovid)\n",
    "\n",
    "test_data_large.reset_index(inplace = True, drop = True)\n",
    "test_target_large = test_data_large['mode'] == 'FM'\n",
    "\n",
    "X_test = feature_creation(test_data_large, crowd_num= 25)\n",
    "\n",
    "X_test_stand = X_test.copy()\n",
    "X_test_stand[numeric_features] = test.transform(X_test_stand[numeric_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dim):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(emb_dim, 500) \n",
    "        self.fc2 = nn.Linear(500, 100)\n",
    "        self.fc3 = nn.Linear(100, 25)\n",
    "        self.fc4 = nn.Linear(25, 25)\n",
    "        self.fc5 = nn.Linear(25, 10)\n",
    "        self.fc6 = nn.Linear(10, 1)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "        \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#general model function for testing hyper parameters\n",
    "\n",
    "\n",
    "def model_train(emb_dim, model, learning_rate, \\\n",
    "                num_epochs, criterion, optimizer, \\\n",
    "                train_loader, val_loader):\n",
    "    \n",
    "\n",
    "\n",
    "    loss_vals = []\n",
    "    acc_est = []\n",
    "    train_est = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (data, labels) in enumerate(train_loader):        \n",
    "            model.train()\n",
    "            data_batch, label_batch = data, labels\n",
    "            label_batch = torch.reshape(label_batch, (-1,1))\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data_batch.float())\n",
    "            loss = criterion(outputs, label_batch.float())\n",
    "            loss_vals.append(loss/labels.size(0))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "    \n",
    "            # validate every 100 iterations\n",
    "            if i > 0 and i % 1000 == 0:\n",
    "                # validate\n",
    "                train_acc = test_model(train_loader, model)\n",
    "                train_est.append(train_acc)\n",
    "                val_acc = test_model(val_loader, model)\n",
    "                acc_est.append(val_acc)\n",
    "                #loss_vals.append(test_model_LOSS(train_loader,model))\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}],Train Acc:{}, Validation Acc: {}'.format( \n",
    "                           epoch+1, num_epochs, i+1, len(train_loader), train_acc, val_acc))\n",
    "                \n",
    "            \n",
    "    \n",
    "    return loss_vals, train_est, acc_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_prep(X_train_stand,Y_data)\n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=10,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "val_data = data_prep(X_val_stand,Y_data_val)\n",
    "valloader = torch.utils.data.DataLoader(val_data, batch_size=10,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "\n",
    "#test_data_nn = data_loader(X_test_stand,test_target_large)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for use in feature importance\n",
    "pickle.dump(X_train_stand, open(data_pickles + \"X_train_stand.p\", \"wb\"))\n",
    "pickle.dump(X_val_stand, open(data_pickles + \"X_val_stand.p\", \"wb\"))\n",
    "pickle.dump(Y_data, open(data_pickles + \"Y_data.p\", \"wb\"))\n",
    "pickle.dump(Y_data_val, open(data_pickles + \"Y_data_val.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final Model\n",
    "num_epochs = 8# number epoch to train\n",
    "learning_rates = .0001\n",
    "emb_dim = 121\n",
    "\n",
    "loss_performance = []\n",
    "acc_performance = []\n",
    "\n",
    "model = Net(emb_dim)\n",
    "    \n",
    "criterion = torch.nn.BCEWithLogitsLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rates)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=.1)\n",
    "    \n",
    "loss_vals, train_est, acc_est = model_train(emb_dim = emb_dim, model = model, learning_rate = learning_rates, \\\n",
    "                                 num_epochs = num_epochs, criterion = criterion, optimizer = optimizer, \\\n",
    "                                 train_loader = trainloader, val_loader = valloader)\n",
    "    \n",
    "loss_performance.append(loss_vals)\n",
    "acc_performance.append(test_model(trainloader, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info('NN Ran')\n",
    "torch.save(model.state_dict(),models + \"model_large.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
