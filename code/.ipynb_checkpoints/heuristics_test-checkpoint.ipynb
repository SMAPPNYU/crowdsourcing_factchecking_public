{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author of code: William Godel \n",
    "\n",
    "Date: 07/02\n",
    "\n",
    "Purpose: to calculate the heuristic performance of crowds on test data\n",
    "\n",
    "## Data IN: \n",
    "\n",
    "test_data_large.csv\n",
    "\n",
    "test_data_large_covid.csv\n",
    "\n",
    "test_data_large_noncovid.csv\n",
    "\n",
    "train_data_large.csv\n",
    "\n",
    "val_data_large.csv\n",
    "\n",
    "train_data_large_covid.csv\n",
    "\n",
    "val_data_large_covid.csv\n",
    "\n",
    "train_data_large_noncovid.csv\n",
    "\n",
    "val_data_large_noncovid.csv\n",
    "\n",
    "\n",
    "## Data OUT: \n",
    "\n",
    "\n",
    "crowd_size_binary_test.p\n",
    "\n",
    "partisan_crowd_size_binary_test.p\n",
    "\n",
    "unanimous_crowd_size_binary_test.p\n",
    "\n",
    "unanimous_partisan_crowd_size_binary_test.p\n",
    "\n",
    "bayes_crowds_types_test.p\n",
    "\n",
    "Machine: My laptop or Imac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook evaluates various transparent heuristic models - applied to TEST set /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "laptop = True\n",
    "import pickle\n",
    "\n",
    "from functions import bayes_binary, bayes_probs,crowd_mode,eval_comp,quick_graph, full_evaluation, crowd_mode, see_data, array_return, count_ideo_func\n",
    "\n",
    "\n",
    "from path import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data = pd.read_csv(\"train_data_large.csv\")\n",
    "#test_data = pd.read_csv(\"test_data_large.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_data = pd.read_csv(prepared_data + \"test_data_large.csv\")\n",
    "test_data_covid = pd.read_csv(prepared_data + \"test_data_large_covid.csv\")\n",
    "test_data_noncovid = pd.read_csv(prepared_data + \"test_data_large_noncovid.csv\")\n",
    "\n",
    "combined_data_fakesource_test = test_data.copy()\n",
    "combined_data_fakesource_test = combined_data_fakesource_test.append(test_data_covid)\n",
    "combined_data_fakesource_test = combined_data_fakesource_test.append(test_data_noncovid)\n",
    "\n",
    "train_data = pd.read_csv(prepared_data + \"train_data_large.csv\")\n",
    "val_data = pd.read_csv(prepared_data + \"val_data_large.csv\")\n",
    "\n",
    "train_data_covid = pd.read_csv(prepared_data + \"train_data_large_covid.csv\")\n",
    "val_data_covid = pd.read_csv(prepared_data + \"val_data_large_covid.csv\")\n",
    "\n",
    "train_data_noncovid = pd.read_csv(prepared_data + \"train_data_large_noncovid.csv\")\n",
    "val_data_noncovid = pd.read_csv(prepared_data + \"val_data_large_noncovid.csv\")\n",
    "\n",
    "\n",
    "combined_data_fakesource_train = train_data.copy()\n",
    "combined_data_fakesource_train = combined_data_fakesource_train.append(val_data)\n",
    "combined_data_fakesource_train = combined_data_fakesource_train.append(train_data_covid)\n",
    "combined_data_fakesource_train = combined_data_fakesource_train.append(val_data_covid)\n",
    "combined_data_fakesource_train = combined_data_fakesource_train.append(train_data_noncovid)\n",
    "combined_data_fakesource_train = combined_data_fakesource_train.append(val_data_noncovid)\n",
    "\n",
    "combined_data_fakesource_train.reset_index(inplace = True, drop = True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Identifying fake source\n",
    "mod_series = combined_data_fakesource_train[\"article_num\"] % 5 \n",
    "train_data_fakesource = combined_data_fakesource_train[mod_series.between(1, 3)].copy()\n",
    "train_data_fakesource.reset_index(inplace = True, drop = True)\n",
    "\n",
    "mod_serie_test = combined_data_fakesource_test[\"article_num\"] % 5 \n",
    "test_data_fakesource = combined_data_fakesource_test[mod_serie_test.between(1, 3)].copy()\n",
    "test_data_fakesource.reset_index(inplace = True, drop = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crowd size simulation\n",
    "\n",
    "## Only Questionable sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categories\n",
    "\n",
    "For false according to FC:\n",
    "\n",
    "1. fcmodefalse = pure mode false or not\n",
    "2. rmfalse = robust mode false or not\n",
    "3. allfalse = all false or not\n",
    "4. anyfalse = any false set or not\n",
    "5. notrue = no true said against it or not\n",
    "\n",
    "## and then reverse for true\n",
    "\n",
    "Crowd Mode types:\n",
    "\n",
    "1. origmode = pure crowd mode\n",
    "2. nocnd = cnd discarded\n",
    "3. modefm = all cnd counted as fm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### constructing the combos of interest\n",
    "\n",
    "fc_types = ['fcmodefalse',\"rmfalse\",\"allfalse\",\"anyfalse\",\"notrue\",\"alltrue\",'fcmodetrue',\"rmtrue\",\"anytrue\",\"nofalse\"]\n",
    "crowd_types = [\"origmode\",\"nocnd\",\"modefm\"]\n",
    "\n",
    "types_to_cols= {\"allfalse\":\"all_false_set\",\n",
    "              \"anyfalse\":\"any_false_set\",\n",
    "              \"notrue\":\"no_true\",\n",
    "              \"alltrue\":\"all_true_set\",\n",
    "              \"anytrue\":\"any_true_set\",\n",
    "              \"nofalse\":\"no_false\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the dictionary to save it all in\n",
    "crowd_size_binary = {}\n",
    "\n",
    "for fc_type_it in fc_types:\n",
    "    \n",
    "    for crowd_type_it in crowd_types:\n",
    "        \n",
    "        key = fc_type_it + \"_\" + crowd_type_it\n",
    "        \n",
    "        crowd_size_binary[key] = []\n",
    "\n",
    "\n",
    "min_crowd = 1\n",
    "max_crowd = 26\n",
    "\n",
    "data_df = test_data_fakesource\n",
    "\n",
    "for crowd_size in range(min_crowd,max_crowd):\n",
    "\n",
    "    #Doing the same for all binary types\n",
    "    for this_type in fc_types:\n",
    "        \n",
    "        #one subset\n",
    "        data_subset = data_df.iloc[:,:crowd_size]\n",
    "        \n",
    "        all_info = full_evaluation(data_subset,data_df,this_type,crowd_types[0])\n",
    "        crowd_size_binary[this_type + \"_\" + crowd_types[0]].append(all_info)\n",
    "\n",
    "       \n",
    "        all_info = full_evaluation(data_subset,data_df,this_type,crowd_types[1])\n",
    "        crowd_size_binary[this_type + \"_\" + crowd_types[1]].append(all_info)\n",
    "\n",
    "\n",
    "        all_info = full_evaluation(data_subset,data_df,this_type,crowd_types[2])\n",
    "        crowd_size_binary[this_type + \"_\" + crowd_types[2]].append(all_info)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pickle.dump(crowd_size_binary, open( heuristic_data + 'crowd_size_binary_test.p', \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partisan crowds\n",
    "\n",
    "### Crowds of 3,6, 9, 12, 15\n",
    "\n",
    "for each row - find the partisan cols\n",
    "then count answers along the axis\n",
    "\n",
    "\n",
    "notes:\n",
    "\n",
    "make a note for jonathan\n",
    "\n",
    "follow up on misleading or false "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, identify rows that are eligibe up to 15.\n",
    "\n",
    "#columns with ideology\n",
    "ideo_cols = [\"ideology_resp_\" +str(x) for x in range(0,30)]\n",
    "resp_cols = [\"resp_cat_\" +str(x) for x in range(0,30)]\n",
    "\n",
    "\n",
    "#want to only keep rows with sufficient partisan diversity\n",
    "col_ideo_counts = test_data_fakesource.loc[:,ideo_cols].apply(Counter, axis = 1)\n",
    "ideo_keeper_rows = col_ideo_counts.apply(count_ideo_func)\n",
    "\n",
    "#subsetting to those with the minimum number\n",
    "test_data_fakesource_partisan = test_data_fakesource[ideo_keeper_rows].copy()\n",
    "test_data_fakesource_partisan.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the dictionary to save it all in\n",
    "partisan_crowd_size_binary = {}\n",
    "\n",
    "for fc_type_it in fc_types:\n",
    "    \n",
    "    for crowd_type_it in crowd_types:\n",
    "        \n",
    "        key = fc_type_it + \"_\" + crowd_type_it\n",
    "        \n",
    "        partisan_crowd_size_binary[key] = []\n",
    "\n",
    "\n",
    "min_crowd = 1\n",
    "max_crowd = 6\n",
    "\n",
    "data_df = test_data_fakesource_partisan\n",
    "\n",
    "for crowd_size in range(min_crowd,max_crowd):\n",
    "\n",
    "    #Doing the same for all binary types\n",
    "    for this_type in fc_types:\n",
    "        \n",
    "        #one subset\n",
    "        \n",
    "        #creating a dataframe of what to keep\n",
    "\n",
    "        mask1 = data_df[ideo_cols].apply(array_return, length = crowd_size, axis = 1)\n",
    "        new_df = pd.DataFrame.from_dict(dict(zip(mask1.index, mask1.values))).T\n",
    "        \n",
    "        \n",
    "        response_df = data_df[resp_cols].to_numpy()\n",
    "        reponsonse_counter = np.ma.masked_array(data = response_df, mask = ~new_df.to_numpy())\n",
    "        answers_counter  = pd.DataFrame(reponsonse_counter).apply(Counter, axis = 1)\n",
    "    \n",
    "        data_subset = answers_counter\n",
    "        \n",
    "        all_info = full_evaluation(data_subset,data_df,this_type,crowd_types[0], do_count1 = True)\n",
    "        partisan_crowd_size_binary[this_type + \"_\" + crowd_types[0]].append(all_info)\n",
    "\n",
    "       \n",
    "        all_info = full_evaluation(data_subset,data_df,this_type,crowd_types[1], do_count1 = True)\n",
    "        partisan_crowd_size_binary[this_type + \"_\" + crowd_types[1]].append(all_info)\n",
    "\n",
    "\n",
    "        all_info = full_evaluation(data_subset,data_df,this_type,crowd_types[2], do_count1 = True)\n",
    "        partisan_crowd_size_binary[this_type + \"_\" + crowd_types[2]].append(all_info)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(partisan_crowd_size_binary, open( heuristic_data + 'partisan_crowd_size_binary_test.p', \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unaniminity Rule\n",
    "# first - random crowds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the dictionary to save it all in\n",
    "unanimous_crowd_size_binary = {}\n",
    "\n",
    "for fc_type_it in fc_types:\n",
    "\n",
    "    key = fc_type_it + \"_unanimous\" \n",
    "\n",
    "    unanimous_crowd_size_binary[key] = []\n",
    "\n",
    "\n",
    "min_crowd = 2\n",
    "max_crowd = 10\n",
    "\n",
    "data_df = test_data_fakesource\n",
    "\n",
    "for crowd_size in range(min_crowd,max_crowd):\n",
    "\n",
    "    #Doing the same for all binary types\n",
    "    for this_type in fc_types:\n",
    "        \n",
    "        #one subset\n",
    "        data_subset = data_df.iloc[:,:crowd_size]\n",
    "        \n",
    "        all_info = full_evaluation(data_subset,data_df,this_type,\"unan\")\n",
    "        unanimous_crowd_size_binary[this_type + \"_unanimous\"].append(all_info)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(unanimous_crowd_size_binary, open( heuristic_data + 'unanimous_crowd_size_binary_test.p', \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now - parisan crowds but unanimity rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the dictionary to save it all in\n",
    "unanimous_partisan_crowd_size_binary = {}\n",
    "\n",
    "for fc_type_it in fc_types:\n",
    "\n",
    "    key = fc_type_it + \"_unanimous\" \n",
    "\n",
    "    unanimous_partisan_crowd_size_binary[key] = []\n",
    "\n",
    "\n",
    "\n",
    "min_crowd = 1\n",
    "max_crowd = 4 #this is a multiple\n",
    "\n",
    "data_df = test_data_fakesource\n",
    "\n",
    "for crowd_size in range(min_crowd,max_crowd):\n",
    "\n",
    "    #Doing the same for all binary types\n",
    "    for this_type in fc_types:\n",
    "        \n",
    "        #one subset\n",
    "        \n",
    "        #creating a dataframe of what to keep\n",
    "        mask1 = data_df[ideo_cols].apply(array_return, length = crowd_size, axis = 1)\n",
    "        new_df = pd.DataFrame.from_dict(dict(zip(mask1.index, mask1.values))).T\n",
    "        \n",
    "        response_df = data_df[resp_cols].to_numpy()\n",
    "        reponsonse_counter = np.ma.masked_array(data = response_df, mask = ~new_df.to_numpy())\n",
    "        answers_counter  = pd.DataFrame(reponsonse_counter).apply(Counter, axis = 1)\n",
    "    \n",
    "        data_subset = answers_counter\n",
    "        \n",
    "        all_info = full_evaluation(data_subset,data_df,this_type,\"unan\", do_count1 = True)\n",
    "        unanimous_partisan_crowd_size_binary[this_type + \"_unanimous\"].append(all_info)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(unanimous_partisan_crowd_size_binary, open( heuristic_data + 'unanimous_partisan_crowd_size_binary_test.p', \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_crowds_types = {}\n",
    "\n",
    "min_crowd = 10\n",
    "max_crowd = 25 \n",
    "\n",
    "for fc_type_it in fc_types:\n",
    "\n",
    "    key = fc_type_it + \"_bayes\" \n",
    "\n",
    "    bayes_crowds_types[key] = []\n",
    "    \n",
    "\n",
    "data_df = train_data_fakesource\n",
    "test_df = test_data_fakesource\n",
    "    \n",
    "for crowd_size in range(min_crowd,max_crowd+1):\n",
    "    \n",
    "    for this_type in fc_types:\n",
    "        \n",
    "        all_info = full_evaluation(data_df,data_df,this_type,crowd_types[2], \\\n",
    "                                    test_data = test_df, bayes = this_type, num_resp = crowd_size)\n",
    "        bayes_crowds_types[this_type + \"_bayes\"].append(all_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(bayes_crowds_types, open( heuristic_data + 'bayes_crowds_types_test.p', \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
