{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author of code: William Godel \n",
    "\n",
    "Date: 07/02\n",
    "\n",
    "Purpose: to train ML models for larger crowds data, and build select crowds data\n",
    "\n",
    "## Data IN: \n",
    "\n",
    "train_data_large.csv\n",
    "\n",
    "val_data_large.csv\n",
    "\n",
    "train_data_large_covid.csv\n",
    "\n",
    "val_data_large_covid.csv\n",
    "\n",
    "train_data_large_noncovid.csv\n",
    "\n",
    "val_data_large_noncovid.csv\n",
    "\n",
    "\n",
    "## Data OUT:\n",
    "\n",
    "### models\n",
    "\n",
    "grid_rf_large.p\n",
    "\n",
    "grid_en_large.p\n",
    "\n",
    "model_large.pt\n",
    "\n",
    "Machine: My laptop or Imac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "laptop = True\n",
    "import pickle\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(filename='ML_large.log',level=logging.DEBUG)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import ElasticNet, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "from ml_functions import feature_creation, conf_eval, data_prep, test_model\n",
    "\n",
    "from functions import count_mode, bayes_probs, bayes_binary\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#crowd_size, feature_transform\n",
    "\n",
    "from path import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(prepared_data + \"train_data_large.csv\")\n",
    "val_data = pd.read_csv(prepared_data + \"val_data_large.csv\")\n",
    "\n",
    "\n",
    "train_data_covid = pd.read_csv(prepared_data + \"train_data_large_covid.csv\")\n",
    "val_data_covid = pd.read_csv(prepared_data + \"val_data_large_covid.csv\")\n",
    "\n",
    "\n",
    "train_data_noncovid = pd.read_csv(prepared_data + \"train_data_large_noncovid.csv\")\n",
    "val_data_noncovid = pd.read_csv(prepared_data + \"val_data_large_noncovid.csv\")\n",
    "\n",
    "\n",
    "\n",
    "train_data_large = train_data.append(train_data_covid)\n",
    "train_data_large = train_data_large.append(train_data_noncovid)\n",
    "\n",
    "train_data_large.reset_index(inplace = True, drop = True)\n",
    "\n",
    "\n",
    "val_data_large = val_data.append(val_data_covid)\n",
    "val_data_large = val_data_large.append(val_data_noncovid)\n",
    "\n",
    "val_data_large.reset_index(inplace = True, drop = True)\n",
    "\n",
    "\n",
    "train_target_large = train_data_large['mode'] == 'FM'\n",
    "val_target_large = val_data_large['mode'] == 'FM'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building ML pipelines:\n",
    "\n",
    "1. Elastic net \n",
    "2. Random Forest\n",
    "3. Gradient Boosted Trees\n",
    "4. NN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the data\n",
    "X_data = feature_creation(train_data_large, crowd_num = 25)\n",
    "Y_data = train_target_large\n",
    "\n",
    "X_data_val = feature_creation(val_data_large, crowd_num = 25)\n",
    "Y_data_val = val_target_large\n",
    "\n",
    "X_data_all = X_data.append(X_data_val)\n",
    "Y_data_all = Y_data.append(Y_data_val)\n",
    "\n",
    "X_data_all.reset_index(inplace = True, drop = True)\n",
    "Y_data_all.reset_index(inplace = True, drop = True)\n",
    "\n",
    "fold_list = [-1 for x in range(X_data.shape[0])]\n",
    "fold_list.extend([0 for x in range(X_data_val.shape[0])])\n",
    "\n",
    "test_fold = np.array(fold_list)\n",
    "ps = PredefinedSplit(test_fold)\n",
    "\n",
    "#encoding the label\n",
    "#le = LabelEncoder()\n",
    "#le.fit(Y_data)\n",
    "#Y_data = le.transform(Y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_large = pd.read_csv(prepared_data + \"test_data_large.csv\")\n",
    "\n",
    "test_target_large = test_data_large['mode'] == 'FM'\n",
    "\n",
    "X_data_test = feature_creation(test_data_large, crowd_num = 25)\n",
    "Y_data_test = test_target_large\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeric Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = []\n",
    "\n",
    "resp_var = [x for x in X_data.columns if 'resp_veracity_' in x]\n",
    "new_cols = [x for x in X_data.columns if 'new' in x]\n",
    "\n",
    "#numeric_features.extend(resp_var)\n",
    "numeric_features.extend(resp_var) #rempve this if it doesn't work\n",
    "numeric_features.extend(new_cols)\n",
    "numeric_features.extend([\"crowd_means\",'crowd_median','crowd_full_range', 'crowd_IQR_range', \\\n",
    "                         'crowd_variance', 'crowd_bayes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0])),\n",
       "             error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('preprocessor',\n",
       "                                        ColumnTransformer(n_jobs=None,\n",
       "                                                          remainder='drop',\n",
       "                                                          sparse_threshold=0.3,\n",
       "                                                          transformer_weights=None,\n",
       "                                                          transformers=[('num',\n",
       "                                                                         Pipeline(memory=None,\n",
       "                                                                                  steps=[('scaler',\n",
       "                                                                                          StandardScaler(copy=True,\n",
       "                                                                                                         with_mean=True,\n",
       "                                                                                                         with_s...\n",
       "                                                               verbose=0,\n",
       "                                                               warm_start=False))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'classifier__max_depth': [1, 2, 3, 4, 5, 10, 20, None],\n",
       "                         'classifier__min_samples_leaf': [1, 2, 3, 5, 10],\n",
       "                         'classifier__min_samples_split': [2, 3, 5, 10],\n",
       "                         'classifier__n_estimators': [100, 250, 300, 400, 500]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[('num', numeric_transformer, numeric_features)])\n",
    "\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', RandomForestClassifier())])\n",
    "\n",
    "parameteres = {'classifier__max_depth':[1,2,3,4,5,10,20,None], \\\n",
    "               'classifier__min_samples_split':[2,3,5,10], \\\n",
    "               'classifier__min_samples_leaf':[1,2,3,5,10], \\\n",
    "               'classifier__n_estimators':[100,250,300,400,500]}\n",
    "\n",
    "grid_rf = GridSearchCV(clf, scoring = 'accuracy', param_grid=parameteres, cv=ps)\n",
    "\n",
    "grid_rf.fit(X_data_all,Y_data_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(grid_rf, open(models + \"grid_rf_large.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wpg205/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/wpg205/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/wpg205/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/wpg205/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/wpg205/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/wpg205/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/wpg205/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/wpg205/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/wpg205/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/wpg205/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/wpg205/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/wpg205/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/wpg205/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/wpg205/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/wpg205/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/wpg205/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/wpg205/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/wpg205/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/wpg205/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/wpg205/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/wpg205/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/wpg205/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/wpg205/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/wpg205/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/wpg205/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/wpg205/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0])),\n",
       "             error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('preprocessor',\n",
       "                                        ColumnTransformer(n_jobs=None,\n",
       "                                                          remainder='drop',\n",
       "                                                          sparse_threshold=0.3,\n",
       "                                                          transformer_weights=None,\n",
       "                                                          transformers=[('num',\n",
       "                                                                         Pipeline(memory=None,\n",
       "                                                                                  steps=[('scaler',\n",
       "                                                                                          StandardScaler(copy=True,\n",
       "                                                                                                         with_mean=True,\n",
       "                                                                                                         with_s...\n",
       "                                                           multi_class='warn',\n",
       "                                                           n_jobs=None,\n",
       "                                                           penalty='elasticnet',\n",
       "                                                           random_state=None,\n",
       "                                                           solver='saga',\n",
       "                                                           tol=0.0001,\n",
       "                                                           verbose=0,\n",
       "                                                           warm_start=False))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'classifier__C': [0.1, 0.5, 1, 5, 10, 20],\n",
       "                         'classifier__l1_ratio': [0.9, 0.8, 0.05, 0.2, 0.15,\n",
       "                                                  0.1, 1]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Elastic Net\n",
    "numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[('num', numeric_transformer, numeric_features)])\n",
    "\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', LogisticRegression(penalty = 'elasticnet', solver = 'saga'))])\n",
    "\n",
    "parameteres = {'classifier__l1_ratio':[0.9,.8,.05,.2,.15,0.1,1], 'classifier__C':[.1,.5,1,5,10,20]}\n",
    "\n",
    "grid_en = GridSearchCV(clf, scoring = 'accuracy', param_grid=parameteres, cv=ps)\n",
    "\n",
    "grid_en.fit(X_data_all,Y_data_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(grid_en, open(models + \"grid_en_large.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the data\n",
    "\n",
    "\n",
    "# Preparing the data\n",
    "#X_data = feature_creation(train_data_fakesource.iloc[:,:])\n",
    "#Y_data = train_target_large_lowcred[:]\n",
    "\n",
    "\n",
    "test = StandardScaler()\n",
    "test.fit(X_data[numeric_features])\n",
    "\n",
    "#training data\n",
    "X_train_stand = X_data.copy()\n",
    "X_train_stand[numeric_features] = test.transform(X_train_stand[numeric_features])\n",
    "\n",
    "#val data\n",
    "X_val_stand = X_data_val.copy()\n",
    "X_val_stand[numeric_features] = test.transform(X_val_stand[numeric_features])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(prepared_data + \"test_data_large.csv\")\n",
    "test_data_covid = pd.read_csv(prepared_data + \"test_data_large_covid.csv\")\n",
    "test_data_noncovid = pd.read_csv(prepared_data + \"test_data_large_noncovid.csv\")\n",
    "\n",
    "test_data_large = test_data.append(test_data_covid)\n",
    "test_data_large = test_data_large.append(test_data_noncovid)\n",
    "\n",
    "test_data_large.reset_index(inplace = True, drop = True)\n",
    "test_target_large = test_data_large['mode'] == 'FM'\n",
    "\n",
    "X_test = feature_creation(test_data_large, crowd_num= 25)\n",
    "\n",
    "X_test_stand = X_test.copy()\n",
    "X_test_stand[numeric_features] = test.transform(X_test_stand[numeric_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dim):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(emb_dim, 500) \n",
    "        self.fc2 = nn.Linear(500, 100)\n",
    "        self.fc3 = nn.Linear(100, 25)\n",
    "        self.fc4 = nn.Linear(25, 25)\n",
    "        self.fc5 = nn.Linear(25, 10)\n",
    "        self.fc6 = nn.Linear(10, 1)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "        \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#general model function for testing hyper parameters\n",
    "\n",
    "\n",
    "def model_train(emb_dim, model, learning_rate, \\\n",
    "                num_epochs, criterion, optimizer, \\\n",
    "                train_loader, val_loader):\n",
    "    \n",
    "\n",
    "\n",
    "    loss_vals = []\n",
    "    acc_est = []\n",
    "    train_est = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (data, labels) in enumerate(train_loader):        \n",
    "            model.train()\n",
    "            data_batch, label_batch = data, labels\n",
    "            label_batch = torch.reshape(label_batch, (-1,1))\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data_batch.float())\n",
    "            loss = criterion(outputs, label_batch.float())\n",
    "            loss_vals.append(loss/labels.size(0))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "    \n",
    "            # validate every 100 iterations\n",
    "            if i > 0 and i % 1000 == 0:\n",
    "                # validate\n",
    "                train_acc = test_model(train_loader, model)\n",
    "                train_est.append(train_acc)\n",
    "                val_acc = test_model(val_loader, model)\n",
    "                acc_est.append(val_acc)\n",
    "                #loss_vals.append(test_model_LOSS(train_loader,model))\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}],Train Acc:{}, Validation Acc: {}'.format( \n",
    "                           epoch+1, num_epochs, i+1, len(train_loader), train_acc, val_acc))\n",
    "                \n",
    "            \n",
    "    \n",
    "    return loss_vals, train_est, acc_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_prep(X_train_stand,Y_data)\n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=10,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "val_data = data_prep(X_val_stand,Y_data_val)\n",
    "valloader = torch.utils.data.DataLoader(val_data, batch_size=10,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "\n",
    "#test_data_nn = data_loader(X_test_stand,test_target_large)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for use in feature importance\n",
    "pickle.dump(X_train_stand, open(data_pickles + \"X_train_stand.p\", \"wb\"))\n",
    "pickle.dump(X_val_stand, open(data_pickles + \"X_val_stand.p\", \"wb\"))\n",
    "pickle.dump(Y_data, open(data_pickles + \"Y_data.p\", \"wb\"))\n",
    "pickle.dump(Y_data_val, open(data_pickles + \"Y_data_val.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/8], Step: [1001/4300],Train Acc:66.17209302325581, Validation Acc: 57.53333333333333\n",
      "Epoch: [1/8], Step: [2001/4300],Train Acc:67.45116279069768, Validation Acc: 58.19047619047619\n",
      "Epoch: [1/8], Step: [3001/4300],Train Acc:69.46279069767442, Validation Acc: 58.75238095238095\n",
      "Epoch: [1/8], Step: [4001/4300],Train Acc:71.68837209302326, Validation Acc: 61.02857142857143\n",
      "Epoch: [2/8], Step: [1001/4300],Train Acc:73.85116279069767, Validation Acc: 65.43809523809524\n",
      "Epoch: [2/8], Step: [2001/4300],Train Acc:75.25813953488372, Validation Acc: 65.16190476190476\n",
      "Epoch: [2/8], Step: [3001/4300],Train Acc:76.05813953488372, Validation Acc: 69.06666666666666\n",
      "Epoch: [2/8], Step: [4001/4300],Train Acc:76.36976744186046, Validation Acc: 68.2\n",
      "Epoch: [3/8], Step: [1001/4300],Train Acc:75.61395348837209, Validation Acc: 69.03809523809524\n",
      "Epoch: [3/8], Step: [2001/4300],Train Acc:77.1093023255814, Validation Acc: 68.43809523809524\n",
      "Epoch: [3/8], Step: [3001/4300],Train Acc:77.22093023255815, Validation Acc: 69.53333333333333\n",
      "Epoch: [3/8], Step: [4001/4300],Train Acc:77.37209302325581, Validation Acc: 67.23809523809524\n",
      "Epoch: [4/8], Step: [1001/4300],Train Acc:77.40232558139535, Validation Acc: 71.29523809523809\n",
      "Epoch: [4/8], Step: [2001/4300],Train Acc:78.06976744186046, Validation Acc: 69.75238095238095\n",
      "Epoch: [4/8], Step: [3001/4300],Train Acc:77.71860465116279, Validation Acc: 69.96190476190476\n",
      "Epoch: [4/8], Step: [4001/4300],Train Acc:78.48372093023256, Validation Acc: 68.71428571428571\n",
      "Epoch: [5/8], Step: [1001/4300],Train Acc:77.63023255813954, Validation Acc: 70.04761904761905\n",
      "Epoch: [5/8], Step: [2001/4300],Train Acc:78.97674418604652, Validation Acc: 70.22857142857143\n",
      "Epoch: [5/8], Step: [3001/4300],Train Acc:78.53488372093024, Validation Acc: 70.25714285714285\n",
      "Epoch: [5/8], Step: [4001/4300],Train Acc:79.48372093023256, Validation Acc: 71.5904761904762\n",
      "Epoch: [6/8], Step: [1001/4300],Train Acc:79.55581395348837, Validation Acc: 70.91428571428571\n",
      "Epoch: [6/8], Step: [2001/4300],Train Acc:79.16744186046512, Validation Acc: 67.45714285714286\n",
      "Epoch: [6/8], Step: [3001/4300],Train Acc:79.90232558139535, Validation Acc: 70.2\n",
      "Epoch: [6/8], Step: [4001/4300],Train Acc:80.12790697674419, Validation Acc: 69.75238095238095\n",
      "Epoch: [7/8], Step: [1001/4300],Train Acc:80.0093023255814, Validation Acc: 68.5047619047619\n",
      "Epoch: [7/8], Step: [2001/4300],Train Acc:80.56744186046511, Validation Acc: 70.4095238095238\n",
      "Epoch: [7/8], Step: [3001/4300],Train Acc:80.65116279069767, Validation Acc: 70.28571428571429\n",
      "Epoch: [7/8], Step: [4001/4300],Train Acc:80.91162790697675, Validation Acc: 70.70476190476191\n",
      "Epoch: [8/8], Step: [1001/4300],Train Acc:81.19534883720931, Validation Acc: 70.02857142857142\n",
      "Epoch: [8/8], Step: [2001/4300],Train Acc:81.52558139534884, Validation Acc: 69.54285714285714\n",
      "Epoch: [8/8], Step: [3001/4300],Train Acc:81.32558139534883, Validation Acc: 70.4095238095238\n",
      "Epoch: [8/8], Step: [4001/4300],Train Acc:81.85813953488372, Validation Acc: 69.4\n"
     ]
    }
   ],
   "source": [
    "#Final Model\n",
    "num_epochs = 8# number epoch to train\n",
    "learning_rates = .0001\n",
    "emb_dim = 121\n",
    "\n",
    "loss_performance = []\n",
    "acc_performance = []\n",
    "\n",
    "model = Net(emb_dim)\n",
    "    \n",
    "criterion = torch.nn.BCEWithLogitsLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rates)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=.1)\n",
    "    \n",
    "loss_vals, train_est, acc_est = model_train(emb_dim = emb_dim, model = model, learning_rate = learning_rates, \\\n",
    "                                 num_epochs = num_epochs, criterion = criterion, optimizer = optimizer, \\\n",
    "                                 train_loader = trainloader, val_loader = valloader)\n",
    "    \n",
    "loss_performance.append(loss_vals)\n",
    "acc_performance.append(test_model(trainloader, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info('NN Ran')\n",
    "torch.save(model.state_dict(),models + \"model_large.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
